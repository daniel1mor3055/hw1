{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8a71iASq9Gja",
        "outputId": "7e487009-c596-49ce-a7e2-cdc88cd0eade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (2.3.1+cu118)\n",
            "Requirement already satisfied: torchvision in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (0.18.1+cu118)\n",
            "Requirement already satisfied: torchaudio in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (2.3.1+cu118)\n",
            "Requirement already satisfied: filelock in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch) (2024.2.0)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: numpy in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torchvision) (1.26.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torchvision) (10.2.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZmMhUpmj_Jpd",
        "outputId": "ad017c96-a4ad-4197-87af-5fb3c5a6422d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (2.20.0)\n",
            "Requirement already satisfied: wandb in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (0.17.4)\n",
            "Requirement already satisfied: einops in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (0.8.0)\n",
            "Requirement already satisfied: torchdata==0.7.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (0.7.1)\n",
            "Requirement already satisfied: portalocker==2.10.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (2.10.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torchdata==0.7.1) (2.2.2)\n",
            "Requirement already satisfied: requests in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torchdata==0.7.1) (2.32.3)\n",
            "Requirement already satisfied: torch>=2 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torchdata==0.7.1) (2.3.1+cu118)\n",
            "Requirement already satisfied: pywin32>=226 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from portalocker==2.10.0) (306)\n",
            "Requirement already satisfied: filelock in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (1.26.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.2.0)\n",
            "Requirement already satisfied: aiohttp in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (0.24.2)\n",
            "Requirement already satisfied: packaging in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (5.27.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (6.0.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (2.9.0)\n",
            "Requirement already satisfied: setproctitle in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (56.0.0)\n",
            "Requirement already satisfied: typing-extensions in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (4.9.0)\n",
            "Requirement already satisfied: colorama in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
            "Requirement already satisfied: six>=1.4.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from requests->torchdata==0.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from requests->torchdata==0.7.1) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from requests->torchdata==0.7.1) (2024.7.4)\n",
            "Requirement already satisfied: sympy in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch>=2->torchdata==0.7.1) (1.12)\n",
            "Requirement already satisfied: networkx in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch>=2->torchdata==0.7.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch>=2->torchdata==0.7.1) (3.1.3)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch>=2->torchdata==0.7.1) (2021.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: intel-openmp==2021.* in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=2->torchdata==0.7.1) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=2->torchdata==0.7.1) (2021.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from jinja2->torch>=2->torchdata==0.7.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from sympy->torch>=2->torchdata==0.7.1) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install datasets wandb einops torchdata==0.7.1 portalocker==2.10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJYGyX9Yk4eo"
      },
      "source": [
        "Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCQ3ZAGvk-MD"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5AlllYobk6nj"
      },
      "outputs": [],
      "source": [
        "from functools import cached_property\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class CustomLSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, output_dim, finetune=True):\n",
        "        super(CustomLSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_dim = embed_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = vocab_size\n",
        "        self.finetune = finetune\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Create layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            input_dim = embed_dim if i == 0 else hidden_dim\n",
        "            self.layers.append(LSTMLayer(input_dim, hidden_dim))\n",
        "\n",
        "        # Output layer weights and biases\n",
        "        self.Wy = nn.Parameter(torch.empty(output_dim, hidden_dim))\n",
        "        self.by = nn.Parameter(torch.zeros(output_dim, 1))\n",
        "\n",
        "        # Initialize weights using Xavier initialization\n",
        "        nn.init.xavier_uniform_(self.Wy)\n",
        "\n",
        "    def forward(self, texts):\n",
        "        batch_size, seq_len = texts.size()\n",
        "        embedded = self.embedding(texts).permute(1, 2, 0)  # (seq_len, embed_dim, batch_size)\n",
        "\n",
        "        h = [\n",
        "            torch.zeros(self.hidden_dim, batch_size).to(texts.device)\n",
        "            for _ in range(self.num_layers)\n",
        "        ]\n",
        "        c = [\n",
        "            torch.zeros(self.hidden_dim, batch_size).to(texts.device)\n",
        "            for _ in range(self.num_layers)\n",
        "        ]\n",
        "\n",
        "        if not self.finetune:\n",
        "            y = torch.zeros(seq_len, batch_size, self.vocab_size).to(texts.device)  # (seq_len, batch_size, vocab_size)\n",
        "            for t in range(seq_len):\n",
        "                x = embedded[t, :, :]  # (embed_dim, batch_size)\n",
        "                for i, layer in enumerate(self.layers):\n",
        "                    h[i], c[i] = layer(x, h[i], c[i])\n",
        "                    x = h[i]\n",
        "                h_last = h[-1].permute(1, 0)  # (batch_size, hidden_dim)\n",
        "                y[t] = torch.matmul(h_last, self.Wy.t()) + self.by.t()  # (batch_size, vocab_size)\n",
        "\n",
        "            return y.permute(1, 0, 2)\n",
        "        else:\n",
        "            for t in range(seq_len):\n",
        "                x = embedded[t, :, :]  # (embed_dim, batch_size)\n",
        "                for i, layer in enumerate(self.layers):\n",
        "                    h[i], c[i] = layer(x, h[i], c[i])\n",
        "                    x = h[i]\n",
        "\n",
        "            h_last = h[-1].permute(1, 0)  # (batch_size, hidden_dim)\n",
        "            y = torch.matmul(h_last, self.Wy.t()) + self.by.t()  # (batch_size, output_dim)\n",
        "            return y.squeeze(1)\n",
        "\n",
        "    @cached_property\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "class LSTMLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(LSTMLayer, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # LSTM weights and biases\n",
        "        self.Wf = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
        "        self.bf = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
        "        self.Wi = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
        "        self.bi = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
        "        self.Wo = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
        "        self.bo = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
        "        self.Wc = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
        "        self.bc = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
        "\n",
        "        # Initialize weights using Xavier initialization\n",
        "        nn.init.xavier_uniform_(self.Wf)\n",
        "        nn.init.xavier_uniform_(self.Wi)\n",
        "        nn.init.xavier_uniform_(self.Wo)\n",
        "        nn.init.xavier_uniform_(self.Wc)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return torch.tanh(x)\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        concat = torch.cat((h, x), dim=0)  # (hidden_dim + input_dim, batch_size)\n",
        "\n",
        "        ft = self.sigmoid(\n",
        "            torch.matmul(self.Wf, concat) + self.bf\n",
        "        )  # (hidden, batch_size)\n",
        "        it = self.sigmoid(\n",
        "            torch.matmul(self.Wi, concat) + self.bi\n",
        "        )  # (hidden, batch_size)\n",
        "        c_hat = self.tanh(\n",
        "            torch.matmul(self.Wc, concat) + self.bc\n",
        "        )  # (hidden, batch_size)\n",
        "        c = ft * c + it * c_hat  # (hidden, batch_size)\n",
        "        ot = self.sigmoid(\n",
        "            torch.matmul(self.Wo, concat) + self.bo\n",
        "        )  # (hidden, batch_size)\n",
        "        h = ot * self.tanh(c)\n",
        "\n",
        "        return h, c\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwSlNHbilAPX"
      },
      "source": [
        "S4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zaiAg-gmlBVS"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from functools import cached_property\n",
        "\n",
        "import torch\n",
        "from einops import rearrange, repeat\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class DropoutNd(nn.Module):\n",
        "    def __init__(self, p: float = 0.5, tie=True, transposed=True):\n",
        "        \"\"\"\n",
        "        tie: tie dropout mask across sequence lengths (Dropout1d/2d/3d)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if p < 0 or p >= 1:\n",
        "            raise ValueError(\"dropout probability has to be in [0, 1), \" \"but got {}\".format(p))\n",
        "        self.p = p\n",
        "        self.tie = tie\n",
        "        self.transposed = transposed\n",
        "        self.binomial = torch.distributions.binomial.Binomial(probs=1 - self.p)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"X: (batch, dim, lengths...).\"\"\"\n",
        "        if self.training:\n",
        "            if not self.transposed: X = rearrange(X, 'b ... d -> b d ...')\n",
        "            # binomial = torch.distributions.binomial.Binomial(probs=1-self.p) # This is incredibly slow because of CPU -> GPU copying\n",
        "            mask_shape = X.shape[:2] + (1,) * (X.ndim - 2) if self.tie else X.shape\n",
        "            # mask = self.binomial.sample(mask_shape)\n",
        "            mask = torch.rand(*mask_shape, device=X.device) < 1. - self.p\n",
        "            X = X * mask * (1.0 / (1 - self.p))\n",
        "            if not self.transposed: X = rearrange(X, 'b d ... -> b ... d')\n",
        "            return X\n",
        "        return X\n",
        "\n",
        "\n",
        "class S4DKernel(nn.Module):\n",
        "    \"\"\"Generate convolution kernel from diagonal SSM parameters.\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim, N=64, dt_min=0.001, dt_max=0.1, lr=None):\n",
        "        super().__init__()\n",
        "        # Generate dt\n",
        "        H = hidden_dim\n",
        "        log_dt = torch.rand(H) * (\n",
        "                math.log(dt_max) - math.log(dt_min)\n",
        "        ) + math.log(dt_min)\n",
        "\n",
        "        C = torch.randn(H, N // 2, dtype=torch.cfloat)\n",
        "        self.C = nn.Parameter(torch.view_as_real(C))\n",
        "        self.register(\"log_dt\", log_dt, lr)\n",
        "\n",
        "        log_A_real = torch.log(0.5 * torch.ones(H, N // 2))\n",
        "        A_imag = math.pi * repeat(torch.arange(N // 2), 'n -> h n', h=H)\n",
        "        self.register(\"log_A_real\", log_A_real, lr)\n",
        "        self.register(\"A_imag\", A_imag, lr)\n",
        "\n",
        "    def forward(self, L):\n",
        "        \"\"\"\n",
        "        returns: (..., c, L) where c is number of channels (default 1)\n",
        "        \"\"\"\n",
        "\n",
        "        # Materialize parameters\n",
        "        dt = torch.exp(self.log_dt)  # (H)\n",
        "        C = torch.view_as_complex(self.C)  # (H N)\n",
        "        A = -torch.exp(self.log_A_real) + 1j * self.A_imag  # (H N)\n",
        "\n",
        "        # Vandermonde multiplication\n",
        "        dtA = A * dt.unsqueeze(-1)  # (H N)\n",
        "        K = dtA.unsqueeze(-1) * torch.arange(L, device=A.device)  # (H N L)\n",
        "        C = C * (torch.exp(dtA) - 1.) / A\n",
        "        K = 2 * torch.einsum('hn, hnl -> hl', C, torch.exp(K)).real\n",
        "\n",
        "        return K\n",
        "\n",
        "    def register(self, name, tensor, lr=None):\n",
        "        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n",
        "\n",
        "        if lr == 0.0:\n",
        "            self.register_buffer(name, tensor)\n",
        "        else:\n",
        "            self.register_parameter(name, nn.Parameter(tensor))\n",
        "\n",
        "            optim = {\"weight_decay\": 0.0}\n",
        "            if lr is not None: optim[\"lr\"] = lr\n",
        "            setattr(getattr(self, name), \"_optim\", optim)\n",
        "\n",
        "\n",
        "class S4D(nn.Module):\n",
        "    def __init__(self, hidden_dim, d_state=64, dropout=0.0, transposed=True, **kernel_args):\n",
        "        super().__init__()\n",
        "\n",
        "        self.h = hidden_dim\n",
        "        self.n = d_state\n",
        "        self.output_dim = self.h\n",
        "        self.transposed = transposed\n",
        "\n",
        "        self.D = nn.Parameter(torch.randn(self.h))\n",
        "\n",
        "        # SSM Kernel\n",
        "        self.kernel = S4DKernel(self.h, N=self.n, **kernel_args)\n",
        "\n",
        "        # Pointwise\n",
        "        self.activation = nn.GELU()\n",
        "        # dropout_fn = nn.Dropout2d # NOTE: bugged in PyTorch 1.11\n",
        "        dropout_fn = DropoutNd\n",
        "        self.dropout = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()\n",
        "\n",
        "        # position-wise output transform to mix features\n",
        "        self.output_linear = nn.Sequential(\n",
        "            nn.Conv1d(self.h, 2 * self.h, kernel_size=1),\n",
        "            nn.GLU(dim=-2),\n",
        "        )\n",
        "\n",
        "    def forward(self, u, **kwargs):  # absorbs return_output and transformer src mask\n",
        "        \"\"\" Input and output shape (B, H, L) \"\"\"\n",
        "        if not self.transposed: u = u.transpose(-1, -2)\n",
        "        L = u.size(-1)\n",
        "\n",
        "        # Compute SSM Kernel\n",
        "        k = self.kernel(L=L)  # (H L)\n",
        "\n",
        "        # Convolution\n",
        "        k_f = torch.fft.rfft(k, n=2 * L)  # (H L)\n",
        "        u_f = torch.fft.rfft(u, n=2 * L)  # (B H L)\n",
        "        y = torch.fft.irfft(u_f * k_f, n=2 * L)[..., :L]  # (B H L)\n",
        "\n",
        "        # Compute D term in state space equation - essentially a skip connection\n",
        "        y = y + u * self.D.unsqueeze(-1)\n",
        "\n",
        "        y = self.dropout(self.activation(y))\n",
        "        y = self.output_linear(y)\n",
        "        if not self.transposed: y = y.transpose(-1, -2)\n",
        "        return y, None\n",
        "\n",
        "\n",
        "class S4Model(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embed_dim,\n",
        "            vocab_size,\n",
        "            output_dim,\n",
        "            hidden_dim=256,\n",
        "            num_layers=4,\n",
        "            dropout=0.1,\n",
        "            lr=0.001,\n",
        "            prenorm=False,\n",
        "            finetune=True\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.prenorm = prenorm\n",
        "\n",
        "        # Linear encoder (embed_dim = 1 for grayscale and 3 for RGB)\n",
        "        self.encoder = nn.Linear(embed_dim, hidden_dim)\n",
        "\n",
        "        # Stack S4 layers as residual blocks\n",
        "        self.s4_layers = nn.ModuleList()\n",
        "        self.norms = nn.ModuleList()\n",
        "        self.dropouts = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.s4_layers.append(\n",
        "                S4D(hidden_dim, dropout=dropout, transposed=True, lr=lr)\n",
        "            )\n",
        "            self.norms.append(nn.LayerNorm(hidden_dim))\n",
        "            self.dropouts.append(nn.Dropout(dropout))\n",
        "\n",
        "        # Linear decoder\n",
        "        self.decoder = nn.Linear(hidden_dim, output_dim)\n",
        "        self.finetune = finetune\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Input x is shape (B, L, embed_dim)\n",
        "        \"\"\"\n",
        "        x = self.embedding(x)  # -> (B, L, embed_dim)\n",
        "\n",
        "        x = self.encoder(x)  # (B, L, embed_dim) -> (B, L, hidden_dim)\n",
        "\n",
        "        x = x.transpose(-1, -2)  # (B, L, hidden_dim) -> (B, hidden_dim, L)\n",
        "        for layer, norm, dropout in zip(self.s4_layers, self.norms, self.dropouts):\n",
        "            # Each iteration of this loop will map (B, hidden_dim, L) -> (B, hidden_dim, L)\n",
        "\n",
        "            z = x\n",
        "            if self.prenorm:\n",
        "                # Prenorm\n",
        "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
        "\n",
        "            # Apply S4 block: we ignore the state input and output\n",
        "            z, _ = layer(z)\n",
        "\n",
        "            # Dropout on the output of the S4 block\n",
        "            z = dropout(z)\n",
        "\n",
        "            # Residual connection\n",
        "            x = z + x\n",
        "\n",
        "            if not self.prenorm:\n",
        "                # Postnorm\n",
        "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
        "\n",
        "        x = x.transpose(-1, -2)\n",
        "\n",
        "        # Pooling: average pooling over the sequence length\n",
        "        if self.finetune:\n",
        "            x = x.mean(dim=1)\n",
        "            # Decode the outputs\n",
        "            x = self.decoder(x)  # (B, hidden_dim) -> (B, output_dim)\n",
        "            return x.squeeze(-1)\n",
        "\n",
        "        else:\n",
        "            x = self.decoder(x)  # (B, hidden_dim) -> (B, output_dim)\n",
        "            return x.squeeze(-1).permute(1, 0, 2)\n",
        "\n",
        "    @cached_property\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ8vUdpWlDFH"
      },
      "source": [
        "Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "geTAyd9vlFE_"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from functools import cached_property\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.scale = 1 / math.sqrt(self.head_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_length, embed_dim = x.size()\n",
        "\n",
        "        qkv = self.qkv_proj(x)  # (batch_size, seq_length, embed_dim * 3)\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_length, 3 * head_dim)\n",
        "\n",
        "        q, k, v = qkv.chunk(3, dim=-1)  # each will be (batch_size, num_heads, seq_length, head_dim)\n",
        "\n",
        "        attn_scores = torch.einsum('bnqd,bnkd->bnqk', q,\n",
        "                                   k) * self.scale  # (batch_size, num_heads, seq_length, seq_length)\n",
        "\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn_probs = nn.functional.softmax(attn_scores, dim=-1)  # (batch_size, num_heads, seq_length, seq_length)\n",
        "\n",
        "        attn_output = torch.einsum('bnqk,bnvd->bnqd', attn_probs, v)  # (batch_size, num_heads, seq_length, head_dim)\n",
        "        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim)  # (batch_size, seq_length, embed_dim)\n",
        "\n",
        "        output = self.o_proj(attn_output)  # (batch_size, seq_length, embed_dim)\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.attention(x, mask)\n",
        "        x = self.layernorm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feedforward(x)\n",
        "        x = self.layernorm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "\n",
        "class CustomTransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, hidden_dim, output_dim, dropout=0.1,\n",
        "                 finetune=True):\n",
        "        super(CustomTransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embed_dim)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, hidden_dim, dropout)\n",
        "            for _ in range(num_layers)])\n",
        "        self.fc_out = nn.Linear(embed_dim, output_dim)\n",
        "        self.finetune = finetune\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).T\n",
        "        mask = mask.int()\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size()\n",
        "        mask = self.generate_square_subsequent_mask(seq_len).to(x.device)\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, mask)\n",
        "        if self.finetune:\n",
        "            x = x.mean(dim=1)\n",
        "            x = self.fc_out(x)\n",
        "            return x.squeeze(1)\n",
        "        else:\n",
        "            x = self.fc_out(x)\n",
        "            return x.squeeze(2)\n",
        "\n",
        "    @cached_property\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "# Setup basic configuration for logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout)  # Ensure logs are directed to stdout\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def setup_logger(name=__name__):\n",
        "    logger = logging.getLogger(name)\n",
        "    return logger\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kEeq75VlJ8m"
      },
      "source": [
        "Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgFmWPi0lMqG"
      },
      "source": [
        "IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTyW58kvlLAA",
        "outputId": "f1189865-84e9-4cd0-9110-b5eb333d8e24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Studies\\RajaCourse\\hw1\\.venv\\lib\\site-packages\\torchtext\\datasets\\__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "d:\\Studies\\RajaCourse\\hw1\\.venv\\lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.datasets import IMDB\n",
        "\n",
        "\n",
        "class IMDBDataset:\n",
        "    # Initialize BPE tokenizer\n",
        "    tokenizer = Tokenizer(models.BPE())\n",
        "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "\n",
        "    trainer = trainers.BpeTrainer(\n",
        "        vocab_size=10000, special_tokens=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\"]\n",
        "    )\n",
        "\n",
        "    @staticmethod\n",
        "    def yield_texts(data_iter):\n",
        "        for _, text in data_iter:\n",
        "            yield text\n",
        "\n",
        "    @staticmethod\n",
        "    def get_tokenizer_and_vocab():\n",
        "        train_iter = IMDB(split=\"train\")\n",
        "        IMDBDataset.tokenizer.train_from_iterator(\n",
        "            IMDBDataset.yield_texts(train_iter), IMDBDataset.trainer\n",
        "        )\n",
        "        return IMDBDataset.tokenizer\n",
        "\n",
        "    @staticmethod\n",
        "    def text_pipeline(text, tokenizer):\n",
        "        return tokenizer.encode(text).ids\n",
        "\n",
        "    @staticmethod\n",
        "    def label_pipeline(label):\n",
        "        return label - 1\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_batch(batch, tokenizer):\n",
        "        label_list, text_list = [], []\n",
        "        for _label, _text in batch:\n",
        "            label_list.append(IMDBDataset.label_pipeline(_label))\n",
        "            processed_text = torch.tensor(\n",
        "                IMDBDataset.text_pipeline(_text, tokenizer), dtype=torch.int64\n",
        "            )\n",
        "            text_list.append(processed_text)\n",
        "        return torch.tensor(label_list, dtype=torch.int64), pad_sequence(\n",
        "            text_list, padding_value=tokenizer.token_to_id(\"<pad>\"), batch_first=True\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def get_dataloaders(batch_size):\n",
        "        tokenizer = IMDBDataset.tokenizer\n",
        "        train_iter, test_iter = IMDB(split=\"train\"), IMDB(split=\"test\")\n",
        "        train_dataloader = DataLoader(\n",
        "            list(train_iter),\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=lambda x: IMDBDataset.collate_batch(x, tokenizer),\n",
        "        )\n",
        "        test_dataloader = DataLoader(\n",
        "            list(test_iter),\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=lambda x: IMDBDataset.collate_batch(x, tokenizer),\n",
        "        )\n",
        "        return train_dataloader, test_dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qPZsIz1lO09"
      },
      "source": [
        "Wikitext Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VXA027TSlQ-g"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-27 23:32:37 - datasets - INFO - PyTorch version 2.3.1+cu118 available.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Studies\\RajaCourse\\hw1\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from logger import setup_logger\n",
        "\n",
        "# Initialize BPE tokenizer\n",
        "\n",
        "logger = setup_logger(__name__)\n",
        "\n",
        "\n",
        "class WikiTextDataset(Dataset):\n",
        "    tokenizer = Tokenizer(models.BPE())\n",
        "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "\n",
        "    trainer = trainers.BpeTrainer(\n",
        "        vocab_size=10000, special_tokens=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\"]\n",
        "    )\n",
        "\n",
        "    def __init__(self, split, tokenizer):\n",
        "        self.dataset = load_dataset(\n",
        "            \"Salesforce/wikitext\", \"wikitext-103-raw-v1\", split=split\n",
        "        ).filter(lambda x: x[\"text\"].strip() != \"\")\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset[idx][\"text\"]\n",
        "\n",
        "    @staticmethod\n",
        "    def yield_texts(data_iter):\n",
        "        for text in data_iter:\n",
        "            yield text\n",
        "\n",
        "    @staticmethod\n",
        "    def get_tokenizer_and_vocab():\n",
        "        tokenizer_file = \"wikitext_tokenizer.json\"\n",
        "\n",
        "        # Check if the tokenizer file already exists\n",
        "        if os.path.exists(tokenizer_file):\n",
        "            logger.info(\"Tokenizer loaded from file.\")\n",
        "            return Tokenizer.from_file(tokenizer_file)\n",
        "\n",
        "        train_iter = load_dataset(\n",
        "            \"Salesforce/wikitext\", \"wikitext-103-raw-v1\", split=\"train\"\n",
        "        ).filter(lambda x: x[\"text\"].strip() != \"\")[\"text\"]\n",
        "        WikiTextDataset.tokenizer.train_from_iterator(\n",
        "            WikiTextDataset.yield_texts(train_iter), WikiTextDataset.trainer\n",
        "        )\n",
        "\n",
        "        # Save the tokenizer to a file\n",
        "        WikiTextDataset.tokenizer.save(tokenizer_file)\n",
        "        logger.info(\"Tokenizer trained and saved to file.\")\n",
        "\n",
        "        return WikiTextDataset.tokenizer\n",
        "\n",
        "    @staticmethod\n",
        "    def text_pipeline(text, tokenizer):\n",
        "        return tokenizer.encode(text).ids\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_batch(batch, tokenizer):\n",
        "        text_list = []\n",
        "        for _text in batch:\n",
        "            processed_text = torch.tensor(\n",
        "                WikiTextDataset.text_pipeline(_text, tokenizer), dtype=torch.int64\n",
        "            )\n",
        "            text_list.append(processed_text)\n",
        "        # Dummy labels for wikitext\n",
        "        return torch.zeros(len(text_list), dtype=torch.int64), pad_sequence(\n",
        "            text_list, padding_value=tokenizer.token_to_id(\"<pad>\"), batch_first=True\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def get_dataloaders(batch_size):\n",
        "        tokenizer = WikiTextDataset.tokenizer\n",
        "        train_dataset = WikiTextDataset(split=\"train\", tokenizer=tokenizer)\n",
        "        test_dataset = WikiTextDataset(split=\"test\", tokenizer=tokenizer)\n",
        "\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=lambda x: WikiTextDataset.collate_batch(x, tokenizer),\n",
        "        )\n",
        "        test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=lambda x: WikiTextDataset.collate_batch(x, tokenizer),\n",
        "        )\n",
        "        return train_dataloader, test_dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV_iO_pzlYhW"
      },
      "source": [
        "Train Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjDcD1ODlcOk"
      },
      "source": [
        "Pretrain Train_Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iWiuyv5Jlarg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import wandb\n",
        "\n",
        "\n",
        "def pretrain_train(model, dataloader, criterion, optimizer, device, epoch, logger, use_wandb):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (_, texts) in enumerate(dataloader):\n",
        "        texts = texts.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_texts = texts[:, :-1]\n",
        "        target_texts = texts[:, 1:]\n",
        "\n",
        "        output = model(input_texts)\n",
        "        loss = criterion(output.reshape(-1, output.size(-1)), target_texts.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            logger.info(\n",
        "                f\"Train Epoch: {epoch + 1} [{batch_idx * len(texts)}/{len(dataloader.dataset)} \"\n",
        "                f\"({100. * batch_idx / len(dataloader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
        "            )\n",
        "            if use_wandb:\n",
        "                wandb.log({\"train_batch_loss\": loss.item()})\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    logger.info(f\"====> Epoch: {epoch + 1} Average loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def pretrain_evaluate(model, dataloader, criterion, device, logger, use_wandb):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (_, texts) in enumerate(dataloader):\n",
        "            texts = texts.to(device)\n",
        "\n",
        "            # Shift the target texts by one time step for the decoder input\n",
        "            input_texts = texts[:, :-1]\n",
        "            target_texts = texts[:, 1:]\n",
        "\n",
        "            output = model(input_texts)\n",
        "            loss = criterion(output.reshape(-1, output.size(-1)), target_texts.reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                logger.info(\n",
        "                    f\"Eval Batch: {batch_idx + 1}/{len(dataloader)}\\tLoss: {loss.item():.6f}\"\n",
        "                )\n",
        "            if use_wandb:\n",
        "                wandb.log({\"eval_batch_loss\": loss.item()})\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    logger.info(f\"====> Test set loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ0ksxkwlf8j"
      },
      "source": [
        "Train Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UEc-mxPolhQZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import wandb\n",
        "\n",
        "\n",
        "def finetune_train(model, dataloader, criterion, optimizer, device, epoch, logger, use_wandb):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (labels, texts) in enumerate(dataloader):\n",
        "        labels, texts = labels.to(device), texts.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(texts)\n",
        "        loss = criterion(output, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            logger.info(\n",
        "                f\"Train Epoch: {epoch + 1} [{batch_idx * len(labels)}/{len(dataloader.dataset)} \"\n",
        "                f\"({100. * batch_idx / len(dataloader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
        "            )\n",
        "            if use_wandb:\n",
        "                wandb.log({\"train_batch_loss\": loss.item()})\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    logger.info(f\"====> Epoch: {epoch + 1} Average loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def finetune_evaluate(model, dataloader, criterion, device, logger, use_wandb):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (labels, texts) in enumerate(dataloader):\n",
        "            labels, texts = labels.to(device), texts.to(device)\n",
        "            output = model(texts)\n",
        "            loss = criterion(output, labels.float())\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                logger.info(\n",
        "                    f\"Eval Batch: {batch_idx + 1}/{len(dataloader)}\\tLoss: {loss.item():.6f}\"\n",
        "                )\n",
        "            if use_wandb:\n",
        "                wandb.log({\"eval_batch_loss\": loss.item()})\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    logger.info(f\"====> Test set loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVZWzbollybG"
      },
      "source": [
        "Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbGe5G3hlGCI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tuYV0cXGlz7_"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"models\": {\n",
        "        \"lstm\": {\n",
        "            \"embed_dim\": 16,\n",
        "            \"hidden_dim\": 128,\n",
        "            \"num_layers\": 2,\n",
        "            \"output_dim\": 1\n",
        "        },\n",
        "        \"s4\": {\n",
        "            \"embed_dim\": 24,\n",
        "            \"hidden_dim\": 128,\n",
        "            \"output_dim\": 1,\n",
        "            \"num_layers\": 2\n",
        "        },\n",
        "        \"transformer\": {\n",
        "            \"embed_dim\": 32,\n",
        "            \"num_heads\": 2,\n",
        "            \"hidden_dim\": 128,\n",
        "            \"output_dim\": 1,\n",
        "            \"num_layers\": 4\n",
        "        }\n",
        "    },\n",
        "    \"run_parameters\": {\n",
        "        \"batch_size\": 1,\n",
        "        \"n_epochs\": 2,\n",
        "        \"learning_rate\": 0.001\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22InhBYbl6Ye"
      },
      "source": [
        "Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9DhoXleNl7Vh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def replace_final_layer(model, config, model_name, device):\n",
        "    hidden_dim = config[\"models\"][model_name][\"hidden_dim\"]\n",
        "    embed_dim = config[\"models\"][model_name][\"embed_dim\"]\n",
        "    if model_name == \"lstm\":\n",
        "        output_dim = 1\n",
        "        model.Wy = torch.nn.Parameter(torch.empty(output_dim, hidden_dim).to(device))\n",
        "        model.by = torch.nn.Parameter(torch.zeros(output_dim, 1).to(device))\n",
        "        torch.nn.init.xavier_uniform_(model.Wy)\n",
        "    elif model_name == \"s4\":\n",
        "        output_dim = 1\n",
        "        model.decoder = torch.nn.Linear(hidden_dim, output_dim).to(device)\n",
        "    else:\n",
        "        output_dim = 1\n",
        "        model.fc_out = torch.nn.Linear(embed_dim, output_dim).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayIrsAyEl89H"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1sey0XXal912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-27 23:32:41 - __main__ - INFO - parameter_cnt: 365953\n",
            "2024-07-27 23:32:41 - __main__ - INFO - Starting training...\n",
            "2024-07-27 23:32:41 - __main__ - INFO - Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.716576\n",
            "2024-07-27 23:32:45 - __main__ - INFO - Train Epoch: 1 [10/25000 (0%)]\tLoss: 0.919421\n",
            "2024-07-27 23:32:49 - __main__ - INFO - Train Epoch: 1 [20/25000 (0%)]\tLoss: 0.781103\n",
            "2024-07-27 23:32:53 - __main__ - INFO - Train Epoch: 1 [30/25000 (0%)]\tLoss: 0.583926\n",
            "2024-07-27 23:32:57 - __main__ - INFO - Train Epoch: 1 [40/25000 (0%)]\tLoss: 0.573063\n",
            "2024-07-27 23:32:59 - __main__ - INFO - Train Epoch: 1 [50/25000 (0%)]\tLoss: 0.685610\n",
            "2024-07-27 23:33:02 - __main__ - INFO - Train Epoch: 1 [60/25000 (0%)]\tLoss: 0.866625\n",
            "2024-07-27 23:33:06 - __main__ - INFO - Train Epoch: 1 [70/25000 (0%)]\tLoss: 0.556777\n",
            "2024-07-27 23:33:09 - __main__ - INFO - Train Epoch: 1 [80/25000 (0%)]\tLoss: 0.836691\n",
            "2024-07-27 23:33:14 - __main__ - INFO - Train Epoch: 1 [90/25000 (0%)]\tLoss: 0.525147\n",
            "2024-07-27 23:33:17 - __main__ - INFO - Train Epoch: 1 [100/25000 (0%)]\tLoss: 0.431122\n",
            "2024-07-27 23:33:21 - __main__ - INFO - Train Epoch: 1 [110/25000 (0%)]\tLoss: 0.519616\n",
            "2024-07-27 23:33:25 - __main__ - INFO - Train Epoch: 1 [120/25000 (0%)]\tLoss: 0.829108\n",
            "2024-07-27 23:33:28 - __main__ - INFO - Train Epoch: 1 [130/25000 (1%)]\tLoss: 0.513876\n",
            "2024-07-27 23:33:31 - __main__ - INFO - Train Epoch: 1 [140/25000 (1%)]\tLoss: 0.486448\n",
            "2024-07-27 23:33:34 - __main__ - INFO - Train Epoch: 1 [150/25000 (1%)]\tLoss: 0.845790\n",
            "2024-07-27 23:33:37 - __main__ - INFO - Train Epoch: 1 [160/25000 (1%)]\tLoss: 0.692120\n",
            "2024-07-27 23:33:40 - __main__ - INFO - Train Epoch: 1 [170/25000 (1%)]\tLoss: 0.494796\n",
            "2024-07-27 23:33:44 - __main__ - INFO - Train Epoch: 1 [180/25000 (1%)]\tLoss: 0.600204\n",
            "2024-07-27 23:33:47 - __main__ - INFO - Train Epoch: 1 [190/25000 (1%)]\tLoss: 0.752039\n",
            "2024-07-27 23:33:50 - __main__ - INFO - Train Epoch: 1 [200/25000 (1%)]\tLoss: 0.572428\n",
            "2024-07-27 23:33:54 - __main__ - INFO - Train Epoch: 1 [210/25000 (1%)]\tLoss: 0.746741\n",
            "2024-07-27 23:33:56 - __main__ - INFO - Train Epoch: 1 [220/25000 (1%)]\tLoss: 0.533366\n",
            "2024-07-27 23:33:59 - __main__ - INFO - Train Epoch: 1 [230/25000 (1%)]\tLoss: 0.908311\n",
            "2024-07-27 23:34:03 - __main__ - INFO - Train Epoch: 1 [240/25000 (1%)]\tLoss: 0.606198\n",
            "2024-07-27 23:34:05 - __main__ - INFO - Train Epoch: 1 [250/25000 (1%)]\tLoss: 0.907162\n",
            "2024-07-27 23:34:08 - __main__ - INFO - Train Epoch: 1 [260/25000 (1%)]\tLoss: 0.768154\n",
            "2024-07-27 23:34:12 - __main__ - INFO - Train Epoch: 1 [270/25000 (1%)]\tLoss: 0.681272\n",
            "2024-07-27 23:34:15 - __main__ - INFO - Train Epoch: 1 [280/25000 (1%)]\tLoss: 0.780266\n",
            "2024-07-27 23:34:17 - __main__ - INFO - Train Epoch: 1 [290/25000 (1%)]\tLoss: 0.745772\n",
            "2024-07-27 23:34:21 - __main__ - INFO - Train Epoch: 1 [300/25000 (1%)]\tLoss: 0.751863\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 100\u001b[0m\n\u001b[0;32m     97\u001b[0m     train, evaluate \u001b[38;5;241m=\u001b[39m pretrain_train, pretrain_evaluate\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m--> 100\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_wandb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m evaluate(model, test_dataloader, criterion, device, logger, args\u001b[38;5;241m.\u001b[39muse_wandb)\n\u001b[0;32m    102\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[10], line 14\u001b[0m, in \u001b[0;36mfinetune_train\u001b[1;34m(model, dataloader, criterion, optimizer, device, epoch, logger, use_wandb)\u001b[0m\n\u001b[0;32m     12\u001b[0m output \u001b[38;5;241m=\u001b[39m model(texts)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     16\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[1;32md:\\Studies\\RajaCourse\\hw1\\.venv\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Studies\\RajaCourse\\hw1\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Studies\\RajaCourse\\hw1\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "import torch\n",
        "import wandb\n",
        "\n",
        "\n",
        "\n",
        "model_options = list(config[\"models\"].keys())\n",
        "run_types = [\"task\", \"lra_pretrain\", \"wikitext_pretrain\", \"task_finetune_lra_pretrain\", \"task_finetune_wikitext_pretrain\"]\n",
        "class Args:\n",
        "    def __init__(self, model, run_type, use_wandb = False):\n",
        "        self.model = model\n",
        "        self.run_type = run_type\n",
        "        self.use_wandb = use_wandb\n",
        "\n",
        "args = Args(model=\"lstm\", run_type=\"task\", use_wandb=False)\n",
        "\n",
        "model_config = config[\"models\"][args.model]\n",
        "run_parameters = config[\"run_parameters\"]\n",
        "\n",
        "# Setup logging\n",
        "logger = setup_logger(__name__)\n",
        "\n",
        "run_name = f\"run_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{args.model}_{args.run_type}\"\n",
        "\n",
        "batch_size, n_epochs, learning_rate = run_parameters[\"batch_size\"], run_parameters[\"n_epochs\"], run_parameters[\n",
        "    \"learning_rate\"]\n",
        "\n",
        "# Load tokenizer and data loaders\n",
        "if \"wikitext\" in args.run_type and \"finetune\" in args.run_type:\n",
        "    get_tokenizer_and_vocab = WikiTextDataset.get_tokenizer_and_vocab\n",
        "    get_dataloaders = IMDBDataset.get_dataloaders\n",
        "elif \"wikitext\" in args.run_type:\n",
        "    get_tokenizer_and_vocab = WikiTextDataset.get_tokenizer_and_vocab\n",
        "    get_dataloaders = WikiTextDataset.get_dataloaders\n",
        "else:\n",
        "    get_tokenizer_and_vocab = IMDBDataset.get_tokenizer_and_vocab\n",
        "    get_dataloaders = IMDBDataset.get_dataloaders\n",
        "tokenizer = get_tokenizer_and_vocab()\n",
        "train_dataloader, test_dataloader = get_dataloaders(batch_size)\n",
        "\n",
        "# Initialize model, criterion, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "finetune = args.run_type in [\"task\", \"task_finetune_lra_pretrain\", \"task_finetune_wikitext_pretrain\"]\n",
        "\n",
        "model_dic = {\n",
        "    \"lstm\": CustomLSTMModel,\n",
        "    \"s4\": S4Model,\n",
        "    \"transformer\": CustomTransformerModel\n",
        "}\n",
        "\n",
        "if args.run_type == \"task\":\n",
        "    config[\"models\"][args.model][\"output_dim\"] = 1\n",
        "else:\n",
        "    config[\"models\"][args.model][\"output_dim\"] = vocab_size\n",
        "\n",
        "model = model_dic[args.model](vocab_size=vocab_size, **config[\"models\"][args.model], finetune=finetune)\n",
        "model.to(device)\n",
        "\n",
        "pretrained = \"lra_pretrained\" if \"lra\" in args.run_type else \"wikitext_pretrained\"\n",
        "checkpoint_path = f\"{args.model}_{pretrained}.pth\"\n",
        "if finetune and args.run_type != \"task\":\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "    logger.info(f\"Checkpoint loaded from {checkpoint_path}\")\n",
        "    replace_final_layer(model, config, args.model, device)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss() if finetune else torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "logger.info(f\"parameter_cnt: {model.count_parameters}\")\n",
        "\n",
        "if args.use_wandb:\n",
        "    # Initialize WandB\n",
        "    wandb.login(key=\"5fda0926085bc8963be5e43c4e501d992e35abe8\")\n",
        "    wandb.init(project=\"model-comparison\", name=run_name)\n",
        "\n",
        "    # Log hyperparameters and model\n",
        "    wandb.config.update(\n",
        "        {\n",
        "            **{\n",
        "                \"run_name\": run_name,\n",
        "                \"model\": args.model,\n",
        "                \"parameter_cnt\": model.count_parameters,\n",
        "            },\n",
        "            **config[\"models\"][args.model],\n",
        "            **config[\"run_parameters\"]}\n",
        "    )\n",
        "\n",
        "# Training loop\n",
        "logger.info(\"Starting training...\")\n",
        "\n",
        "if finetune:\n",
        "    train, evaluate = finetune_train, finetune_evaluate\n",
        "else:\n",
        "    train, evaluate = pretrain_train, pretrain_evaluate\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_dataloader, criterion, optimizer, device, epoch, logger, args.use_wandb)\n",
        "    test_loss = evaluate(model, test_dataloader, criterion, device, logger, args.use_wandb)\n",
        "    logger.info(f\"Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    if args.use_wandb:\n",
        "        # Log metrics to WandB\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"test_loss\": test_loss\n",
        "        })\n",
        "\n",
        "pretrained = \"lra_pretrained\" if \"lra\" in args.run_type else \"wikitext_pretrained\"\n",
        "checkpoint_path = f\"{args.model}_{pretrained}.pth\"\n",
        "if not finetune:\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    logger.info(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "\n",
        "logger.info(\"Training completed.\")\n",
        "\n",
        "if args.use_wandb:\n",
        "    wandb.finish()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
