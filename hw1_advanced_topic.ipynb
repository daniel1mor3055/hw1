{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "8a71iASq9Gja",
    "outputId": "7e487009-c596-49ce-a7e2-cdc88cd0eade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (2.3.1+cu118)\n",
      "Requirement already satisfied: torchvision in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (0.18.1+cu118)\n",
      "Requirement already satisfied: torchaudio in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (2.3.1+cu118)\n",
      "Requirement already satisfied: filelock in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: numpy in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ZmMhUpmj_Jpd",
    "outputId": "ad017c96-a4ad-4197-87af-5fb3c5a6422d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: wandb in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (0.17.4)\n",
      "Requirement already satisfied: einops in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: torchdata==0.7.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: portalocker==2.10.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torchdata==0.7.1) (2.2.2)\n",
      "Requirement already satisfied: requests in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torchdata==0.7.1) (2.32.3)\n",
      "Requirement already satisfied: torch>=2 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torchdata==0.7.1) (2.3.1+cu118)\n",
      "Requirement already satisfied: pywin32>=226 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from portalocker==2.10.0) (306)\n",
      "Requirement already satisfied: filelock in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (0.24.2)\n",
      "Requirement already satisfied: packaging in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (5.27.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (2.9.0)\n",
      "Requirement already satisfied: setproctitle in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (56.0.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from wandb) (4.9.0)\n",
      "Requirement already satisfied: colorama in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from requests->torchdata==0.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from requests->torchdata==0.7.1) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from requests->torchdata==0.7.1) (2024.7.4)\n",
      "Requirement already satisfied: sympy in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch>=2->torchdata==0.7.1) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch>=2->torchdata==0.7.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch>=2->torchdata==0.7.1) (3.1.3)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from torch>=2->torchdata==0.7.1) (2021.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=2->torchdata==0.7.1) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=2->torchdata==0.7.1) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from jinja2->torch>=2->torchdata==0.7.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\studies\\rajacourse\\hw1\\.venv\\lib\\site-packages (from sympy->torch>=2->torchdata==0.7.1) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install datasets wandb einops torchdata==0.7.1 portalocker==2.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJYGyX9Yk4eo"
   },
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCQ3ZAGvk-MD"
   },
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5AlllYobk6nj"
   },
   "outputs": [],
   "source": [
    "from functools import cached_property\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CustomLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, output_dim, finetune=True):\n",
    "        super(CustomLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.finetune = finetune\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Create layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            input_dim = embed_dim if i == 0 else hidden_dim\n",
    "            self.layers.append(LSTMLayer(input_dim, hidden_dim))\n",
    "\n",
    "        # Output layer weights and biases\n",
    "        self.Wy = nn.Parameter(torch.empty(output_dim, hidden_dim))\n",
    "        self.by = nn.Parameter(torch.zeros(output_dim, 1))\n",
    "\n",
    "        # Initialize weights using Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.Wy)\n",
    "\n",
    "    def forward(self, texts):\n",
    "        batch_size, seq_len = texts.size()\n",
    "        embedded = self.embedding(texts).permute(1, 2, 0)  # (seq_len, embed_dim, batch_size)\n",
    "\n",
    "        h = [\n",
    "            torch.zeros(self.hidden_dim, batch_size).to(texts.device)\n",
    "            for _ in range(self.num_layers)\n",
    "        ]\n",
    "        c = [\n",
    "            torch.zeros(self.hidden_dim, batch_size).to(texts.device)\n",
    "            for _ in range(self.num_layers)\n",
    "        ]\n",
    "\n",
    "        if not self.finetune:\n",
    "            y = torch.zeros(seq_len, batch_size, self.vocab_size).to(texts.device)  # (seq_len, batch_size, vocab_size)\n",
    "            for t in range(seq_len):\n",
    "                x = embedded[t, :, :]  # (embed_dim, batch_size)\n",
    "                for i, layer in enumerate(self.layers):\n",
    "                    h[i], c[i] = layer(x, h[i], c[i])\n",
    "                    x = h[i]\n",
    "                h_last = h[-1].permute(1, 0)  # (batch_size, hidden_dim)\n",
    "                y[t] = torch.matmul(h_last, self.Wy.t()) + self.by.t()  # (batch_size, vocab_size)\n",
    "\n",
    "            return y.permute(1, 0, 2)\n",
    "        else:\n",
    "            for t in range(seq_len):\n",
    "                x = embedded[t, :, :]  # (embed_dim, batch_size)\n",
    "                for i, layer in enumerate(self.layers):\n",
    "                    h[i], c[i] = layer(x, h[i], c[i])\n",
    "                    x = h[i]\n",
    "\n",
    "            h_last = h[-1].permute(1, 0)  # (batch_size, hidden_dim)\n",
    "            y = torch.matmul(h_last, self.Wy.t()) + self.by.t()  # (batch_size, output_dim)\n",
    "            return y.squeeze(1)\n",
    "\n",
    "    @cached_property\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # LSTM weights and biases\n",
    "        self.Wf = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
    "        self.bf = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
    "        self.Wi = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
    "        self.bi = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
    "        self.Wo = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
    "        self.bo = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
    "        self.Wc = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
    "        self.bc = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
    "\n",
    "        # Initialize weights using Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.Wf)\n",
    "        nn.init.xavier_uniform_(self.Wi)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.Wc)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return torch.tanh(x)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        concat = torch.cat((h, x), dim=0)  # (hidden_dim + input_dim, batch_size)\n",
    "\n",
    "        ft = self.sigmoid(\n",
    "            torch.matmul(self.Wf, concat) + self.bf\n",
    "        )  # (hidden, batch_size)\n",
    "        it = self.sigmoid(\n",
    "            torch.matmul(self.Wi, concat) + self.bi\n",
    "        )  # (hidden, batch_size)\n",
    "        c_hat = self.tanh(\n",
    "            torch.matmul(self.Wc, concat) + self.bc\n",
    "        )  # (hidden, batch_size)\n",
    "        c = ft * c + it * c_hat  # (hidden, batch_size)\n",
    "        ot = self.sigmoid(\n",
    "            torch.matmul(self.Wo, concat) + self.bo\n",
    "        )  # (hidden, batch_size)\n",
    "        h = ot * self.tanh(c)\n",
    "\n",
    "        return h, c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwSlNHbilAPX"
   },
   "source": [
    "S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zaiAg-gmlBVS"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import cached_property\n",
    "\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class DropoutNd(nn.Module):\n",
    "    def __init__(self, p: float = 0.5, tie=True, transposed=True):\n",
    "        \"\"\"\n",
    "        tie: tie dropout mask across sequence lengths (Dropout1d/2d/3d)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if p < 0 or p >= 1:\n",
    "            raise ValueError(\"dropout probability has to be in [0, 1), \" \"but got {}\".format(p))\n",
    "        self.p = p\n",
    "        self.tie = tie\n",
    "        self.transposed = transposed\n",
    "        self.binomial = torch.distributions.binomial.Binomial(probs=1 - self.p)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"X: (batch, dim, lengths...).\"\"\"\n",
    "        if self.training:\n",
    "            if not self.transposed: X = rearrange(X, 'b ... d -> b d ...')\n",
    "            # binomial = torch.distributions.binomial.Binomial(probs=1-self.p) # This is incredibly slow because of CPU -> GPU copying\n",
    "            mask_shape = X.shape[:2] + (1,) * (X.ndim - 2) if self.tie else X.shape\n",
    "            # mask = self.binomial.sample(mask_shape)\n",
    "            mask = torch.rand(*mask_shape, device=X.device) < 1. - self.p\n",
    "            X = X * mask * (1.0 / (1 - self.p))\n",
    "            if not self.transposed: X = rearrange(X, 'b d ... -> b ... d')\n",
    "            return X\n",
    "        return X\n",
    "\n",
    "\n",
    "class S4DKernel(nn.Module):\n",
    "    \"\"\"Generate convolution kernel from diagonal SSM parameters.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, N=64, dt_min=0.001, dt_max=0.1, lr=None):\n",
    "        super().__init__()\n",
    "        # Generate dt\n",
    "        H = hidden_dim\n",
    "        log_dt = torch.rand(H) * (\n",
    "                math.log(dt_max) - math.log(dt_min)\n",
    "        ) + math.log(dt_min)\n",
    "\n",
    "        C = torch.randn(H, N // 2, dtype=torch.cfloat)\n",
    "        self.C = nn.Parameter(torch.view_as_real(C))\n",
    "        self.register(\"log_dt\", log_dt, lr)\n",
    "\n",
    "        log_A_real = torch.log(0.5 * torch.ones(H, N // 2))\n",
    "        A_imag = math.pi * repeat(torch.arange(N // 2), 'n -> h n', h=H)\n",
    "        self.register(\"log_A_real\", log_A_real, lr)\n",
    "        self.register(\"A_imag\", A_imag, lr)\n",
    "\n",
    "    def forward(self, L):\n",
    "        \"\"\"\n",
    "        returns: (..., c, L) where c is number of channels (default 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Materialize parameters\n",
    "        dt = torch.exp(self.log_dt)  # (H)\n",
    "        C = torch.view_as_complex(self.C)  # (H N)\n",
    "        A = -torch.exp(self.log_A_real) + 1j * self.A_imag  # (H N)\n",
    "\n",
    "        # Vandermonde multiplication\n",
    "        dtA = A * dt.unsqueeze(-1)  # (H N)\n",
    "        K = dtA.unsqueeze(-1) * torch.arange(L, device=A.device)  # (H N L)\n",
    "        C = C * (torch.exp(dtA) - 1.) / A\n",
    "        K = 2 * torch.einsum('hn, hnl -> hl', C, torch.exp(K)).real\n",
    "\n",
    "        return K\n",
    "\n",
    "    def register(self, name, tensor, lr=None):\n",
    "        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n",
    "\n",
    "        if lr == 0.0:\n",
    "            self.register_buffer(name, tensor)\n",
    "        else:\n",
    "            self.register_parameter(name, nn.Parameter(tensor))\n",
    "\n",
    "            optim = {\"weight_decay\": 0.0}\n",
    "            if lr is not None: optim[\"lr\"] = lr\n",
    "            setattr(getattr(self, name), \"_optim\", optim)\n",
    "\n",
    "\n",
    "class S4D(nn.Module):\n",
    "    def __init__(self, hidden_dim, d_state=64, dropout=0.0, transposed=True, **kernel_args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = hidden_dim\n",
    "        self.n = d_state\n",
    "        self.output_dim = self.h\n",
    "        self.transposed = transposed\n",
    "\n",
    "        self.D = nn.Parameter(torch.randn(self.h))\n",
    "\n",
    "        # SSM Kernel\n",
    "        self.kernel = S4DKernel(self.h, N=self.n, **kernel_args)\n",
    "\n",
    "        # Pointwise\n",
    "        self.activation = nn.GELU()\n",
    "        # dropout_fn = nn.Dropout2d # NOTE: bugged in PyTorch 1.11\n",
    "        dropout_fn = DropoutNd\n",
    "        self.dropout = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()\n",
    "\n",
    "        # position-wise output transform to mix features\n",
    "        self.output_linear = nn.Sequential(\n",
    "            nn.Conv1d(self.h, 2 * self.h, kernel_size=1),\n",
    "            nn.GLU(dim=-2),\n",
    "        )\n",
    "\n",
    "    def forward(self, u, **kwargs):  # absorbs return_output and transformer src mask\n",
    "        \"\"\" Input and output shape (B, H, L) \"\"\"\n",
    "        if not self.transposed: u = u.transpose(-1, -2)\n",
    "        L = u.size(-1)\n",
    "\n",
    "        # Compute SSM Kernel\n",
    "        k = self.kernel(L=L)  # (H L)\n",
    "\n",
    "        # Convolution\n",
    "        k_f = torch.fft.rfft(k, n=2 * L)  # (H L)\n",
    "        u_f = torch.fft.rfft(u, n=2 * L)  # (B H L)\n",
    "        y = torch.fft.irfft(u_f * k_f, n=2 * L)[..., :L]  # (B H L)\n",
    "\n",
    "        # Compute D term in state space equation - essentially a skip connection\n",
    "        y = y + u * self.D.unsqueeze(-1)\n",
    "\n",
    "        y = self.dropout(self.activation(y))\n",
    "        y = self.output_linear(y)\n",
    "        if not self.transposed: y = y.transpose(-1, -2)\n",
    "        return y, None\n",
    "\n",
    "\n",
    "class S4Model(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim,\n",
    "            vocab_size,\n",
    "            output_dim,\n",
    "            hidden_dim=256,\n",
    "            num_layers=4,\n",
    "            dropout=0.1,\n",
    "            lr=0.001,\n",
    "            prenorm=False,\n",
    "            finetune=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.prenorm = prenorm\n",
    "\n",
    "        # Linear encoder (embed_dim = 1 for grayscale and 3 for RGB)\n",
    "        self.encoder = nn.Linear(embed_dim, hidden_dim)\n",
    "\n",
    "        # Stack S4 layers as residual blocks\n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.s4_layers.append(\n",
    "                S4D(hidden_dim, dropout=dropout, transposed=True, lr=lr)\n",
    "            )\n",
    "            self.norms.append(nn.LayerNorm(hidden_dim))\n",
    "            self.dropouts.append(nn.Dropout(dropout))\n",
    "\n",
    "        # Linear decoder\n",
    "        self.decoder = nn.Linear(hidden_dim, output_dim)\n",
    "        self.finetune = finetune\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, embed_dim)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)  # -> (B, L, embed_dim)\n",
    "\n",
    "        x = self.encoder(x)  # (B, L, embed_dim) -> (B, L, hidden_dim)\n",
    "\n",
    "        x = x.transpose(-1, -2)  # (B, L, hidden_dim) -> (B, hidden_dim, L)\n",
    "        for layer, norm, dropout in zip(self.s4_layers, self.norms, self.dropouts):\n",
    "            # Each iteration of this loop will map (B, hidden_dim, L) -> (B, hidden_dim, L)\n",
    "\n",
    "            z = x\n",
    "            if self.prenorm:\n",
    "                # Prenorm\n",
    "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "            # Apply S4 block: we ignore the state input and output\n",
    "            z, _ = layer(z)\n",
    "\n",
    "            # Dropout on the output of the S4 block\n",
    "            z = dropout(z)\n",
    "\n",
    "            # Residual connection\n",
    "            x = z + x\n",
    "\n",
    "            if not self.prenorm:\n",
    "                # Postnorm\n",
    "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # Pooling: average pooling over the sequence length\n",
    "        if self.finetune:\n",
    "            x = x.mean(dim=1)\n",
    "            # Decode the outputs\n",
    "            x = self.decoder(x)  # (B, hidden_dim) -> (B, output_dim)\n",
    "            return x.squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            x = self.decoder(x)  # (B, hidden_dim) -> (B, output_dim)\n",
    "            return x.squeeze(-1).permute(1, 0, 2)\n",
    "\n",
    "    @cached_property\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ8vUdpWlDFH"
   },
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "geTAyd9vlFE_"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import cached_property\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = 1 / math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "\n",
    "        qkv = self.qkv_proj(x)  # (batch_size, seq_length, embed_dim * 3)\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_length, 3 * head_dim)\n",
    "\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # each will be (batch_size, num_heads, seq_length, head_dim)\n",
    "\n",
    "        attn_scores = torch.einsum('bnqd,bnkd->bnqk', q,\n",
    "                                   k) * self.scale  # (batch_size, num_heads, seq_length, seq_length)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_probs = nn.functional.softmax(attn_scores, dim=-1)  # (batch_size, num_heads, seq_length, seq_length)\n",
    "\n",
    "        attn_output = torch.einsum('bnqk,bnvd->bnqd', attn_probs, v)  # (batch_size, num_heads, seq_length, head_dim)\n",
    "        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim)  # (batch_size, seq_length, embed_dim)\n",
    "\n",
    "        output = self.o_proj(attn_output)  # (batch_size, seq_length, embed_dim)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = self.layernorm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, hidden_dim, output_dim, dropout=0.1,\n",
    "                 finetune=True):\n",
    "        super(CustomTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, hidden_dim, dropout)\n",
    "            for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(embed_dim, output_dim)\n",
    "        self.finetune = finetune\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).T\n",
    "        mask = mask.int()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        mask = self.generate_square_subsequent_mask(seq_len).to(x.device)\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, mask)\n",
    "        if self.finetune:\n",
    "            x = x.mean(dim=1)\n",
    "            x = self.fc_out(x)\n",
    "            return x.squeeze(1)\n",
    "        else:\n",
    "            x = self.fc_out(x)\n",
    "            return x.squeeze(2)\n",
    "\n",
    "    @cached_property\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Setup basic configuration for logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)  # Ensure logs are directed to stdout\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def setup_logger(name=__name__):\n",
    "    logger = logging.getLogger(name)\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kEeq75VlJ8m"
   },
   "source": [
    "Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFmWPi0lMqG"
   },
   "source": [
    "IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTyW58kvlLAA",
    "outputId": "f1189865-84e9-4cd0-9110-b5eb333d8e24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Studies\\RajaCourse\\hw1\\.venv\\lib\\site-packages\\torchtext\\datasets\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "d:\\Studies\\RajaCourse\\hw1\\.venv\\lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "\n",
    "class IMDBDataset:\n",
    "    # Initialize BPE tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=10000, special_tokens=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\"]\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def yield_texts(data_iter):\n",
    "        for _, text in data_iter:\n",
    "            yield text\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tokenizer_and_vocab():\n",
    "        train_iter = IMDB(split=\"train\")\n",
    "        IMDBDataset.tokenizer.train_from_iterator(\n",
    "            IMDBDataset.yield_texts(train_iter), IMDBDataset.trainer\n",
    "        )\n",
    "        return IMDBDataset.tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def text_pipeline(text, tokenizer):\n",
    "        return tokenizer.encode(text).ids\n",
    "\n",
    "    @staticmethod\n",
    "    def label_pipeline(label):\n",
    "        return label - 1\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_batch(batch, tokenizer):\n",
    "        label_list, text_list = [], []\n",
    "        for _label, _text in batch:\n",
    "            label_list.append(IMDBDataset.label_pipeline(_label))\n",
    "            processed_text = torch.tensor(\n",
    "                IMDBDataset.text_pipeline(_text, tokenizer), dtype=torch.int64\n",
    "            )\n",
    "            text_list.append(processed_text)\n",
    "        return torch.tensor(label_list, dtype=torch.int64), pad_sequence(\n",
    "            text_list, padding_value=tokenizer.token_to_id(\"<pad>\"), batch_first=True\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_dataloaders(batch_size):\n",
    "        tokenizer = IMDBDataset.tokenizer\n",
    "        train_iter, test_iter = IMDB(split=\"train\"), IMDB(split=\"test\")\n",
    "        train_dataloader = DataLoader(\n",
    "            list(train_iter),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=lambda x: IMDBDataset.collate_batch(x, tokenizer),\n",
    "        )\n",
    "        test_dataloader = DataLoader(\n",
    "            list(test_iter),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=lambda x: IMDBDataset.collate_batch(x, tokenizer),\n",
    "        )\n",
    "        return train_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qPZsIz1lO09"
   },
   "source": [
    "Wikitext Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VXA027TSlQ-g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-27 23:32:37 - datasets - INFO - PyTorch version 2.3.1+cu118 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Studies\\RajaCourse\\hw1\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Initialize BPE tokenizer\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=10000, special_tokens=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\"]\n",
    "    )\n",
    "\n",
    "    def __init__(self, split, tokenizer):\n",
    "        self.dataset = load_dataset(\n",
    "            \"Salesforce/wikitext\", \"wikitext-103-raw-v1\", split=split\n",
    "        ).filter(lambda x: x[\"text\"].strip() != \"\")\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx][\"text\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def yield_texts(data_iter):\n",
    "        for text in data_iter:\n",
    "            yield text\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tokenizer_and_vocab():\n",
    "        tokenizer_file = \"wikitext_tokenizer.json\"\n",
    "\n",
    "        # Check if the tokenizer file already exists\n",
    "        if os.path.exists(tokenizer_file):\n",
    "            logger.info(\"Tokenizer loaded from file.\")\n",
    "            WikiTextDataset.tokenizer = Tokenizer.from_file(tokenizer_file)\n",
    "        else:\n",
    "            train_iter = load_dataset(\n",
    "                \"Salesforce/wikitext\", \"wikitext-103-raw-v1\", split=\"train\"\n",
    "            ).filter(lambda x: x[\"text\"].strip() != \"\")[\"text\"]\n",
    "            WikiTextDataset.tokenizer.train_from_iterator(\n",
    "                WikiTextDataset.yield_texts(train_iter), WikiTextDataset.trainer\n",
    "            )\n",
    "\n",
    "            # Save the tokenizer to a file\n",
    "            WikiTextDataset.tokenizer.save(tokenizer_file)\n",
    "            logger.info(\"Tokenizer trained and saved to file.\")\n",
    "\n",
    "        return WikiTextDataset.tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def text_pipeline(text):\n",
    "        return WikiTextDataset.tokenizer.encode(text).ids\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_batch(batch):\n",
    "        text_list = []\n",
    "        for _text in batch:\n",
    "            processed_text = torch.tensor(\n",
    "                WikiTextDataset.text_pipeline(_text),\n",
    "                dtype=torch.int64,\n",
    "            )\n",
    "            text_list.append(processed_text)\n",
    "        # Dummy labels for wikitext\n",
    "        return torch.zeros(len(text_list), dtype=torch.int64), pad_sequence(\n",
    "            text_list,\n",
    "            padding_value=WikiTextDataset.tokenizer.token_to_id(\"<pad>\"),\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_dataloaders(batch_size):\n",
    "        tokenizer = WikiTextDataset.tokenizer\n",
    "        train_dataset = WikiTextDataset(split=\"train\", tokenizer=tokenizer)\n",
    "        test_dataset = WikiTextDataset(split=\"test\", tokenizer=tokenizer)\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=lambda x: WikiTextDataset.collate_batch(x),\n",
    "        )\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=lambda x: WikiTextDataset.collate_batch(x),\n",
    "        )\n",
    "        return train_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV_iO_pzlYhW"
   },
   "source": [
    "Train Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjDcD1ODlcOk"
   },
   "source": [
    "Pretrain Train_Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iWiuyv5Jlarg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "def pretrain_train(model, dataloader, criterion, optimizer, device, epoch, logger, use_wandb):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (_, texts) in enumerate(dataloader):\n",
    "        texts = texts.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_texts = texts[:, :-1]\n",
    "        target_texts = texts[:, 1:]\n",
    "\n",
    "        output = model(input_texts)\n",
    "        loss = criterion(output.reshape(-1, output.size(-1)), target_texts.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            logger.info(\n",
    "                f\"Train Epoch: {epoch + 1} [{batch_idx * len(texts)}/{len(dataloader.dataset)} \"\n",
    "                f\"({100. * batch_idx / len(dataloader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
    "            )\n",
    "            if use_wandb:\n",
    "                wandb.log({\"train_batch_loss\": loss.item()})\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    logger.info(f\"====> Epoch: {epoch + 1} Average loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def pretrain_evaluate(model, dataloader, criterion, device, logger, use_wandb):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (_, texts) in enumerate(dataloader):\n",
    "            texts = texts.to(device)\n",
    "\n",
    "            # Shift the target texts by one time step for the decoder input\n",
    "            input_texts = texts[:, :-1]\n",
    "            target_texts = texts[:, 1:]\n",
    "\n",
    "            output = model(input_texts)\n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), target_texts.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                logger.info(\n",
    "                    f\"Eval Batch: {batch_idx + 1}/{len(dataloader)}\\tLoss: {loss.item():.6f}\"\n",
    "                )\n",
    "                if use_wandb:\n",
    "                    wandb.log({\"eval_batch_loss\": loss.item()})\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    logger.info(f\"====> Test set loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ0ksxkwlf8j"
   },
   "source": [
    "Train Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UEc-mxPolhQZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "def finetune_train(model, dataloader, criterion, optimizer, device, epoch, logger, use_wandb):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (labels, texts) in enumerate(dataloader):\n",
    "        labels, texts = labels.to(device), texts.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(texts)\n",
    "        loss = criterion(output, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            logger.info(\n",
    "                f\"Train Epoch: {epoch + 1} [{batch_idx * len(labels)}/{len(dataloader.dataset)} \"\n",
    "                f\"({100. * batch_idx / len(dataloader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
    "            )\n",
    "            if use_wandb:\n",
    "                wandb.log({\"train_batch_loss\": loss.item()})\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    logger.info(f\"====> Epoch: {epoch + 1} Average loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def finetune_evaluate(model, dataloader, criterion, device, logger, use_wandb):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (labels, texts) in enumerate(dataloader):\n",
    "            labels, texts = labels.to(device), texts.to(device)\n",
    "            output = model(texts)\n",
    "            loss = criterion(output, labels.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                logger.info(\n",
    "                    f\"Eval Batch: {batch_idx + 1}/{len(dataloader)}\\tLoss: {loss.item():.6f}\"\n",
    "                )\n",
    "                if use_wandb:\n",
    "                    wandb.log({\"eval_batch_loss\": loss.item()})\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    logger.info(f\"====> Test set loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVZWzbollybG"
   },
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbGe5G3hlGCI"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tuYV0cXGlz7_"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"models\": {\n",
    "        \"lstm\": {\n",
    "            \"embed_dim\": 16,\n",
    "            \"hidden_dim\": 128,\n",
    "            \"num_layers\": 2,\n",
    "            \"output_dim\": 1\n",
    "        },\n",
    "        \"s4\": {\n",
    "            \"embed_dim\": 24,\n",
    "            \"hidden_dim\": 128,\n",
    "            \"output_dim\": 1,\n",
    "            \"num_layers\": 2\n",
    "        },\n",
    "        \"transformer\": {\n",
    "            \"embed_dim\": 32,\n",
    "            \"num_heads\": 2,\n",
    "            \"hidden_dim\": 128,\n",
    "            \"output_dim\": 1,\n",
    "            \"num_layers\": 4\n",
    "        }\n",
    "    },\n",
    "    \"run_parameters\": {\n",
    "        \"batch_size\": 1,\n",
    "        \"n_epochs\": 2,\n",
    "        \"learning_rate\": 0.001\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22InhBYbl6Ye"
   },
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9DhoXleNl7Vh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def replace_final_layer(model, config, model_name, device):\n",
    "    hidden_dim = config[\"models\"][model_name][\"hidden_dim\"]\n",
    "    embed_dim = config[\"models\"][model_name][\"embed_dim\"]\n",
    "    if model_name == \"lstm\":\n",
    "        output_dim = 1\n",
    "        model.Wy = torch.nn.Parameter(torch.empty(output_dim, hidden_dim).to(device))\n",
    "        model.by = torch.nn.Parameter(torch.zeros(output_dim, 1).to(device))\n",
    "        torch.nn.init.xavier_uniform_(model.Wy)\n",
    "    elif model_name == \"s4\":\n",
    "        output_dim = 1\n",
    "        model.decoder = torch.nn.Linear(hidden_dim, output_dim).to(device)\n",
    "    else:\n",
    "        output_dim = 1\n",
    "        model.fc_out = torch.nn.Linear(embed_dim, output_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayIrsAyEl89H"
   },
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1sey0XXal912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-27 23:46:11 - __main__ - INFO - Tokenizer loaded from file.\n",
      "2024-07-27 23:46:19 - __main__ - INFO - parameter_cnt: 700816\n",
      "2024-07-27 23:46:19 - __main__ - INFO - Starting training...\n",
      "2024-07-27 23:46:19 - __main__ - INFO - Train Epoch: 1 [0/1165029 (0%)]\tLoss: 9.606491\n",
      "2024-07-27 23:46:19 - __main__ - INFO - Train Epoch: 1 [10/1165029 (0%)]\tLoss: 9.217667\n",
      "2024-07-27 23:46:20 - __main__ - INFO - Train Epoch: 1 [20/1165029 (0%)]\tLoss: 9.117012\n",
      "2024-07-27 23:46:20 - __main__ - INFO - Train Epoch: 1 [30/1165029 (0%)]\tLoss: 8.874863\n",
      "2024-07-27 23:46:20 - __main__ - INFO - Train Epoch: 1 [40/1165029 (0%)]\tLoss: 8.868356\n",
      "2024-07-27 23:46:20 - __main__ - INFO - Train Epoch: 1 [50/1165029 (0%)]\tLoss: 7.333674\n",
      "2024-07-27 23:46:20 - __main__ - INFO - Train Epoch: 1 [60/1165029 (0%)]\tLoss: 8.390303\n",
      "2024-07-27 23:46:20 - __main__ - INFO - Train Epoch: 1 [70/1165029 (0%)]\tLoss: 8.008689\n",
      "2024-07-27 23:46:20 - __main__ - INFO - Train Epoch: 1 [80/1165029 (0%)]\tLoss: 8.199543\n",
      "2024-07-27 23:46:20 - __main__ - INFO - Train Epoch: 1 [90/1165029 (0%)]\tLoss: 8.633648\n",
      "2024-07-27 23:46:20 - __main__ - INFO - Train Epoch: 1 [100/1165029 (0%)]\tLoss: 7.844650\n",
      "2024-07-27 23:46:21 - __main__ - INFO - Train Epoch: 1 [110/1165029 (0%)]\tLoss: 7.651862\n",
      "2024-07-27 23:46:21 - __main__ - INFO - Train Epoch: 1 [120/1165029 (0%)]\tLoss: 7.764535\n",
      "2024-07-27 23:46:21 - __main__ - INFO - Train Epoch: 1 [130/1165029 (0%)]\tLoss: 7.530987\n",
      "2024-07-27 23:46:21 - __main__ - INFO - Train Epoch: 1 [140/1165029 (0%)]\tLoss: 7.872213\n",
      "2024-07-27 23:46:21 - __main__ - INFO - Train Epoch: 1 [150/1165029 (0%)]\tLoss: 6.041000\n",
      "2024-07-27 23:46:21 - __main__ - INFO - Train Epoch: 1 [160/1165029 (0%)]\tLoss: 7.536674\n",
      "2024-07-27 23:46:21 - __main__ - INFO - Train Epoch: 1 [170/1165029 (0%)]\tLoss: 6.896758\n",
      "2024-07-27 23:46:21 - __main__ - INFO - Train Epoch: 1 [180/1165029 (0%)]\tLoss: 7.564233\n",
      "2024-07-27 23:46:21 - __main__ - INFO - Train Epoch: 1 [190/1165029 (0%)]\tLoss: 4.264485\n",
      "2024-07-27 23:46:21 - __main__ - INFO - Train Epoch: 1 [200/1165029 (0%)]\tLoss: 7.947722\n",
      "2024-07-27 23:46:22 - __main__ - INFO - Train Epoch: 1 [210/1165029 (0%)]\tLoss: 4.682191\n",
      "2024-07-27 23:46:22 - __main__ - INFO - Train Epoch: 1 [220/1165029 (0%)]\tLoss: 7.675210\n",
      "2024-07-27 23:46:22 - __main__ - INFO - Train Epoch: 1 [230/1165029 (0%)]\tLoss: 7.426087\n",
      "2024-07-27 23:46:22 - __main__ - INFO - Train Epoch: 1 [240/1165029 (0%)]\tLoss: 7.654650\n",
      "2024-07-27 23:46:22 - __main__ - INFO - Train Epoch: 1 [250/1165029 (0%)]\tLoss: 5.973079\n",
      "2024-07-27 23:46:22 - __main__ - INFO - Train Epoch: 1 [260/1165029 (0%)]\tLoss: 4.229516\n",
      "2024-07-27 23:46:22 - __main__ - INFO - Train Epoch: 1 [270/1165029 (0%)]\tLoss: 7.791431\n",
      "2024-07-27 23:46:22 - __main__ - INFO - Train Epoch: 1 [280/1165029 (0%)]\tLoss: 4.326348\n",
      "2024-07-27 23:46:22 - __main__ - INFO - Train Epoch: 1 [290/1165029 (0%)]\tLoss: 7.126374\n",
      "2024-07-27 23:46:23 - __main__ - INFO - Train Epoch: 1 [300/1165029 (0%)]\tLoss: 7.584177\n",
      "2024-07-27 23:46:23 - __main__ - INFO - Train Epoch: 1 [310/1165029 (0%)]\tLoss: 3.229373\n",
      "2024-07-27 23:46:23 - __main__ - INFO - Train Epoch: 1 [320/1165029 (0%)]\tLoss: 7.252218\n",
      "2024-07-27 23:46:23 - __main__ - INFO - Train Epoch: 1 [330/1165029 (0%)]\tLoss: 7.605441\n",
      "2024-07-27 23:46:23 - __main__ - INFO - Train Epoch: 1 [340/1165029 (0%)]\tLoss: 4.733173\n",
      "2024-07-27 23:46:23 - __main__ - INFO - Train Epoch: 1 [350/1165029 (0%)]\tLoss: 2.282724\n",
      "2024-07-27 23:46:23 - __main__ - INFO - Train Epoch: 1 [360/1165029 (0%)]\tLoss: 9.150894\n",
      "2024-07-27 23:46:23 - __main__ - INFO - Train Epoch: 1 [370/1165029 (0%)]\tLoss: 7.597294\n",
      "2024-07-27 23:46:23 - __main__ - INFO - Train Epoch: 1 [380/1165029 (0%)]\tLoss: 7.460487\n",
      "2024-07-27 23:46:24 - __main__ - INFO - Train Epoch: 1 [390/1165029 (0%)]\tLoss: 7.488537\n",
      "2024-07-27 23:46:24 - __main__ - INFO - Train Epoch: 1 [400/1165029 (0%)]\tLoss: 7.319366\n",
      "2024-07-27 23:46:24 - __main__ - INFO - Train Epoch: 1 [410/1165029 (0%)]\tLoss: 7.414832\n",
      "2024-07-27 23:46:24 - __main__ - INFO - Train Epoch: 1 [420/1165029 (0%)]\tLoss: 7.755068\n",
      "2024-07-27 23:46:24 - __main__ - INFO - Train Epoch: 1 [430/1165029 (0%)]\tLoss: 2.745506\n",
      "2024-07-27 23:46:24 - __main__ - INFO - Train Epoch: 1 [440/1165029 (0%)]\tLoss: 7.360969\n",
      "2024-07-27 23:46:24 - __main__ - INFO - Train Epoch: 1 [450/1165029 (0%)]\tLoss: 7.576470\n",
      "2024-07-27 23:46:24 - __main__ - INFO - Train Epoch: 1 [460/1165029 (0%)]\tLoss: 7.525779\n",
      "2024-07-27 23:46:24 - __main__ - INFO - Train Epoch: 1 [470/1165029 (0%)]\tLoss: 7.248972\n",
      "2024-07-27 23:46:25 - __main__ - INFO - Train Epoch: 1 [480/1165029 (0%)]\tLoss: 7.322106\n",
      "2024-07-27 23:46:25 - __main__ - INFO - Train Epoch: 1 [490/1165029 (0%)]\tLoss: 7.509944\n",
      "2024-07-27 23:46:25 - __main__ - INFO - Train Epoch: 1 [500/1165029 (0%)]\tLoss: 6.537794\n",
      "2024-07-27 23:46:25 - __main__ - INFO - Train Epoch: 1 [510/1165029 (0%)]\tLoss: 7.569593\n",
      "2024-07-27 23:46:25 - __main__ - INFO - Train Epoch: 1 [520/1165029 (0%)]\tLoss: 6.089999\n",
      "2024-07-27 23:46:25 - __main__ - INFO - Train Epoch: 1 [530/1165029 (0%)]\tLoss: 7.694907\n",
      "2024-07-27 23:46:25 - __main__ - INFO - Train Epoch: 1 [540/1165029 (0%)]\tLoss: 7.695817\n",
      "2024-07-27 23:46:25 - __main__ - INFO - Train Epoch: 1 [550/1165029 (0%)]\tLoss: 7.689385\n",
      "2024-07-27 23:46:25 - __main__ - INFO - Train Epoch: 1 [560/1165029 (0%)]\tLoss: 7.180063\n",
      "2024-07-27 23:46:26 - __main__ - INFO - Train Epoch: 1 [570/1165029 (0%)]\tLoss: 7.565395\n",
      "2024-07-27 23:46:26 - __main__ - INFO - Train Epoch: 1 [580/1165029 (0%)]\tLoss: 7.153348\n",
      "2024-07-27 23:46:26 - __main__ - INFO - Train Epoch: 1 [590/1165029 (0%)]\tLoss: 7.719916\n",
      "2024-07-27 23:46:26 - __main__ - INFO - Train Epoch: 1 [600/1165029 (0%)]\tLoss: 7.381768\n",
      "2024-07-27 23:46:26 - __main__ - INFO - Train Epoch: 1 [610/1165029 (0%)]\tLoss: 7.008737\n",
      "2024-07-27 23:46:26 - __main__ - INFO - Train Epoch: 1 [620/1165029 (0%)]\tLoss: 7.105711\n",
      "2024-07-27 23:46:26 - __main__ - INFO - Train Epoch: 1 [630/1165029 (0%)]\tLoss: 4.018732\n",
      "2024-07-27 23:46:26 - __main__ - INFO - Train Epoch: 1 [640/1165029 (0%)]\tLoss: 7.522379\n",
      "2024-07-27 23:46:26 - __main__ - INFO - Train Epoch: 1 [650/1165029 (0%)]\tLoss: 5.460588\n",
      "2024-07-27 23:46:27 - __main__ - INFO - Train Epoch: 1 [660/1165029 (0%)]\tLoss: 3.904728\n",
      "2024-07-27 23:46:27 - __main__ - INFO - Train Epoch: 1 [670/1165029 (0%)]\tLoss: 7.391274\n",
      "2024-07-27 23:46:27 - __main__ - INFO - Train Epoch: 1 [680/1165029 (0%)]\tLoss: 6.700764\n",
      "2024-07-27 23:46:27 - __main__ - INFO - Train Epoch: 1 [690/1165029 (0%)]\tLoss: 7.557678\n",
      "2024-07-27 23:46:27 - __main__ - INFO - Train Epoch: 1 [700/1165029 (0%)]\tLoss: 5.376287\n",
      "2024-07-27 23:46:27 - __main__ - INFO - Train Epoch: 1 [710/1165029 (0%)]\tLoss: 7.394498\n",
      "2024-07-27 23:46:27 - __main__ - INFO - Train Epoch: 1 [720/1165029 (0%)]\tLoss: 7.337369\n",
      "2024-07-27 23:46:27 - __main__ - INFO - Train Epoch: 1 [730/1165029 (0%)]\tLoss: 7.553328\n",
      "2024-07-27 23:46:28 - __main__ - INFO - Train Epoch: 1 [740/1165029 (0%)]\tLoss: 2.751743\n",
      "2024-07-27 23:46:28 - __main__ - INFO - Train Epoch: 1 [750/1165029 (0%)]\tLoss: 7.209204\n",
      "2024-07-27 23:46:28 - __main__ - INFO - Train Epoch: 1 [760/1165029 (0%)]\tLoss: 7.185923\n",
      "2024-07-27 23:46:28 - __main__ - INFO - Train Epoch: 1 [770/1165029 (0%)]\tLoss: 7.323207\n",
      "2024-07-27 23:46:28 - __main__ - INFO - Train Epoch: 1 [780/1165029 (0%)]\tLoss: 5.040838\n",
      "2024-07-27 23:46:28 - __main__ - INFO - Train Epoch: 1 [790/1165029 (0%)]\tLoss: 3.481067\n",
      "2024-07-27 23:46:28 - __main__ - INFO - Train Epoch: 1 [800/1165029 (0%)]\tLoss: 7.413448\n",
      "2024-07-27 23:46:28 - __main__ - INFO - Train Epoch: 1 [810/1165029 (0%)]\tLoss: 7.168958\n",
      "2024-07-27 23:46:28 - __main__ - INFO - Train Epoch: 1 [820/1165029 (0%)]\tLoss: 7.534757\n",
      "2024-07-27 23:46:29 - __main__ - INFO - Train Epoch: 1 [830/1165029 (0%)]\tLoss: 7.264837\n",
      "2024-07-27 23:46:29 - __main__ - INFO - Train Epoch: 1 [840/1165029 (0%)]\tLoss: 6.946973\n",
      "2024-07-27 23:46:29 - __main__ - INFO - Train Epoch: 1 [850/1165029 (0%)]\tLoss: 7.248122\n",
      "2024-07-27 23:46:29 - __main__ - INFO - Train Epoch: 1 [860/1165029 (0%)]\tLoss: 7.743587\n",
      "2024-07-27 23:46:29 - __main__ - INFO - Train Epoch: 1 [870/1165029 (0%)]\tLoss: 7.896550\n",
      "2024-07-27 23:46:29 - __main__ - INFO - Train Epoch: 1 [880/1165029 (0%)]\tLoss: 7.390643\n",
      "2024-07-27 23:46:29 - __main__ - INFO - Train Epoch: 1 [890/1165029 (0%)]\tLoss: 3.637097\n",
      "2024-07-27 23:46:29 - __main__ - INFO - Train Epoch: 1 [900/1165029 (0%)]\tLoss: 7.502160\n",
      "2024-07-27 23:46:29 - __main__ - INFO - Train Epoch: 1 [910/1165029 (0%)]\tLoss: 7.142059\n",
      "2024-07-27 23:46:30 - __main__ - INFO - Train Epoch: 1 [920/1165029 (0%)]\tLoss: 7.350245\n",
      "2024-07-27 23:46:30 - __main__ - INFO - Train Epoch: 1 [930/1165029 (0%)]\tLoss: 7.637251\n",
      "2024-07-27 23:46:30 - __main__ - INFO - Train Epoch: 1 [940/1165029 (0%)]\tLoss: 6.011848\n",
      "2024-07-27 23:46:30 - __main__ - INFO - Train Epoch: 1 [950/1165029 (0%)]\tLoss: 7.495963\n",
      "2024-07-27 23:46:30 - __main__ - INFO - Train Epoch: 1 [960/1165029 (0%)]\tLoss: 7.168905\n",
      "2024-07-27 23:46:30 - __main__ - INFO - Train Epoch: 1 [970/1165029 (0%)]\tLoss: 3.305923\n",
      "2024-07-27 23:46:30 - __main__ - INFO - Train Epoch: 1 [980/1165029 (0%)]\tLoss: 7.143432\n",
      "2024-07-27 23:46:30 - __main__ - INFO - Train Epoch: 1 [990/1165029 (0%)]\tLoss: 7.685458\n",
      "2024-07-27 23:46:30 - __main__ - INFO - Train Epoch: 1 [1000/1165029 (0%)]\tLoss: 7.554985\n",
      "2024-07-27 23:46:31 - __main__ - INFO - Train Epoch: 1 [1010/1165029 (0%)]\tLoss: 7.539179\n",
      "2024-07-27 23:46:31 - __main__ - INFO - Train Epoch: 1 [1020/1165029 (0%)]\tLoss: 4.327928\n",
      "2024-07-27 23:46:31 - __main__ - INFO - Train Epoch: 1 [1030/1165029 (0%)]\tLoss: 7.520066\n",
      "2024-07-27 23:46:31 - __main__ - INFO - Train Epoch: 1 [1040/1165029 (0%)]\tLoss: 3.979635\n",
      "2024-07-27 23:46:31 - __main__ - INFO - Train Epoch: 1 [1050/1165029 (0%)]\tLoss: 7.558089\n",
      "2024-07-27 23:46:31 - __main__ - INFO - Train Epoch: 1 [1060/1165029 (0%)]\tLoss: 7.108147\n",
      "2024-07-27 23:46:31 - __main__ - INFO - Train Epoch: 1 [1070/1165029 (0%)]\tLoss: 5.569829\n",
      "2024-07-27 23:46:31 - __main__ - INFO - Train Epoch: 1 [1080/1165029 (0%)]\tLoss: 7.252152\n",
      "2024-07-27 23:46:32 - __main__ - INFO - Train Epoch: 1 [1090/1165029 (0%)]\tLoss: 7.210413\n",
      "2024-07-27 23:46:32 - __main__ - INFO - Train Epoch: 1 [1100/1165029 (0%)]\tLoss: 7.266565\n",
      "2024-07-27 23:46:32 - __main__ - INFO - Train Epoch: 1 [1110/1165029 (0%)]\tLoss: 7.359934\n",
      "2024-07-27 23:46:32 - __main__ - INFO - Train Epoch: 1 [1120/1165029 (0%)]\tLoss: 7.374449\n",
      "2024-07-27 23:46:32 - __main__ - INFO - Train Epoch: 1 [1130/1165029 (0%)]\tLoss: 7.418478\n",
      "2024-07-27 23:46:32 - __main__ - INFO - Train Epoch: 1 [1140/1165029 (0%)]\tLoss: 7.643154\n",
      "2024-07-27 23:46:32 - __main__ - INFO - Train Epoch: 1 [1150/1165029 (0%)]\tLoss: 3.316573\n",
      "2024-07-27 23:46:32 - __main__ - INFO - Train Epoch: 1 [1160/1165029 (0%)]\tLoss: 7.498403\n",
      "2024-07-27 23:46:32 - __main__ - INFO - Train Epoch: 1 [1170/1165029 (0%)]\tLoss: 7.113122\n",
      "2024-07-27 23:46:32 - __main__ - INFO - Train Epoch: 1 [1180/1165029 (0%)]\tLoss: 6.805288\n",
      "2024-07-27 23:46:33 - __main__ - INFO - Train Epoch: 1 [1190/1165029 (0%)]\tLoss: 7.056663\n",
      "2024-07-27 23:46:33 - __main__ - INFO - Train Epoch: 1 [1200/1165029 (0%)]\tLoss: 7.338176\n",
      "2024-07-27 23:46:33 - __main__ - INFO - Train Epoch: 1 [1210/1165029 (0%)]\tLoss: 7.108662\n",
      "2024-07-27 23:46:33 - __main__ - INFO - Train Epoch: 1 [1220/1165029 (0%)]\tLoss: 7.381103\n",
      "2024-07-27 23:46:33 - __main__ - INFO - Train Epoch: 1 [1230/1165029 (0%)]\tLoss: 7.356167\n",
      "2024-07-27 23:46:33 - __main__ - INFO - Train Epoch: 1 [1240/1165029 (0%)]\tLoss: 4.972810\n",
      "2024-07-27 23:46:33 - __main__ - INFO - Train Epoch: 1 [1250/1165029 (0%)]\tLoss: 7.320380\n",
      "2024-07-27 23:46:33 - __main__ - INFO - Train Epoch: 1 [1260/1165029 (0%)]\tLoss: 7.768885\n",
      "2024-07-27 23:46:34 - __main__ - INFO - Train Epoch: 1 [1270/1165029 (0%)]\tLoss: 7.690884\n",
      "2024-07-27 23:46:34 - __main__ - INFO - Train Epoch: 1 [1280/1165029 (0%)]\tLoss: 3.870001\n",
      "2024-07-27 23:46:34 - __main__ - INFO - Train Epoch: 1 [1290/1165029 (0%)]\tLoss: 7.746522\n",
      "2024-07-27 23:46:34 - __main__ - INFO - Train Epoch: 1 [1300/1165029 (0%)]\tLoss: 7.262945\n",
      "2024-07-27 23:46:34 - __main__ - INFO - Train Epoch: 1 [1310/1165029 (0%)]\tLoss: 7.804677\n",
      "2024-07-27 23:46:34 - __main__ - INFO - Train Epoch: 1 [1320/1165029 (0%)]\tLoss: 7.366995\n",
      "2024-07-27 23:46:34 - __main__ - INFO - Train Epoch: 1 [1330/1165029 (0%)]\tLoss: 2.048961\n",
      "2024-07-27 23:46:34 - __main__ - INFO - Train Epoch: 1 [1340/1165029 (0%)]\tLoss: 7.301972\n",
      "2024-07-27 23:46:34 - __main__ - INFO - Train Epoch: 1 [1350/1165029 (0%)]\tLoss: 7.321987\n",
      "2024-07-27 23:46:35 - __main__ - INFO - Train Epoch: 1 [1360/1165029 (0%)]\tLoss: 7.741609\n",
      "2024-07-27 23:46:35 - __main__ - INFO - Train Epoch: 1 [1370/1165029 (0%)]\tLoss: 5.782906\n",
      "2024-07-27 23:46:35 - __main__ - INFO - Train Epoch: 1 [1380/1165029 (0%)]\tLoss: 7.467142\n",
      "2024-07-27 23:46:35 - __main__ - INFO - Train Epoch: 1 [1390/1165029 (0%)]\tLoss: 7.334322\n",
      "2024-07-27 23:46:35 - __main__ - INFO - Train Epoch: 1 [1400/1165029 (0%)]\tLoss: 6.582172\n",
      "2024-07-27 23:46:35 - __main__ - INFO - Train Epoch: 1 [1410/1165029 (0%)]\tLoss: 7.120857\n",
      "2024-07-27 23:46:35 - __main__ - INFO - Train Epoch: 1 [1420/1165029 (0%)]\tLoss: 7.145961\n",
      "2024-07-27 23:46:35 - __main__ - INFO - Train Epoch: 1 [1430/1165029 (0%)]\tLoss: 7.575338\n",
      "2024-07-27 23:46:35 - __main__ - INFO - Train Epoch: 1 [1440/1165029 (0%)]\tLoss: 7.029534\n",
      "2024-07-27 23:46:36 - __main__ - INFO - Train Epoch: 1 [1450/1165029 (0%)]\tLoss: 3.579129\n",
      "2024-07-27 23:46:36 - __main__ - INFO - Train Epoch: 1 [1460/1165029 (0%)]\tLoss: 7.454143\n",
      "2024-07-27 23:46:36 - __main__ - INFO - Train Epoch: 1 [1470/1165029 (0%)]\tLoss: 7.311896\n",
      "2024-07-27 23:46:36 - __main__ - INFO - Train Epoch: 1 [1480/1165029 (0%)]\tLoss: 7.454360\n",
      "2024-07-27 23:46:36 - __main__ - INFO - Train Epoch: 1 [1490/1165029 (0%)]\tLoss: 7.291934\n",
      "2024-07-27 23:46:36 - __main__ - INFO - Train Epoch: 1 [1500/1165029 (0%)]\tLoss: 8.474432\n",
      "2024-07-27 23:46:36 - __main__ - INFO - Train Epoch: 1 [1510/1165029 (0%)]\tLoss: 7.458632\n",
      "2024-07-27 23:46:36 - __main__ - INFO - Train Epoch: 1 [1520/1165029 (0%)]\tLoss: 3.611934\n",
      "2024-07-27 23:46:37 - __main__ - INFO - Train Epoch: 1 [1530/1165029 (0%)]\tLoss: 7.241104\n",
      "2024-07-27 23:46:37 - __main__ - INFO - Train Epoch: 1 [1540/1165029 (0%)]\tLoss: 7.229555\n",
      "2024-07-27 23:46:37 - __main__ - INFO - Train Epoch: 1 [1550/1165029 (0%)]\tLoss: 7.442449\n",
      "2024-07-27 23:46:37 - __main__ - INFO - Train Epoch: 1 [1560/1165029 (0%)]\tLoss: 7.435541\n",
      "2024-07-27 23:46:37 - __main__ - INFO - Train Epoch: 1 [1570/1165029 (0%)]\tLoss: 2.041469\n",
      "2024-07-27 23:46:37 - __main__ - INFO - Train Epoch: 1 [1580/1165029 (0%)]\tLoss: 7.647647\n",
      "2024-07-27 23:46:37 - __main__ - INFO - Train Epoch: 1 [1590/1165029 (0%)]\tLoss: 7.342904\n",
      "2024-07-27 23:46:37 - __main__ - INFO - Train Epoch: 1 [1600/1165029 (0%)]\tLoss: 7.178014\n",
      "2024-07-27 23:46:37 - __main__ - INFO - Train Epoch: 1 [1610/1165029 (0%)]\tLoss: 3.518070\n",
      "2024-07-27 23:46:38 - __main__ - INFO - Train Epoch: 1 [1620/1165029 (0%)]\tLoss: 3.403259\n",
      "2024-07-27 23:46:38 - __main__ - INFO - Train Epoch: 1 [1630/1165029 (0%)]\tLoss: 7.466329\n",
      "2024-07-27 23:46:38 - __main__ - INFO - Train Epoch: 1 [1640/1165029 (0%)]\tLoss: 1.888093\n",
      "2024-07-27 23:46:38 - __main__ - INFO - Train Epoch: 1 [1650/1165029 (0%)]\tLoss: 7.339572\n",
      "2024-07-27 23:46:38 - __main__ - INFO - Train Epoch: 1 [1660/1165029 (0%)]\tLoss: 6.970280\n",
      "2024-07-27 23:46:38 - __main__ - INFO - Train Epoch: 1 [1670/1165029 (0%)]\tLoss: 7.432706\n",
      "2024-07-27 23:46:38 - __main__ - INFO - Train Epoch: 1 [1680/1165029 (0%)]\tLoss: 7.573946\n",
      "2024-07-27 23:46:38 - __main__ - INFO - Train Epoch: 1 [1690/1165029 (0%)]\tLoss: 5.154866\n",
      "2024-07-27 23:46:38 - __main__ - INFO - Train Epoch: 1 [1700/1165029 (0%)]\tLoss: 7.478122\n",
      "2024-07-27 23:46:39 - __main__ - INFO - Train Epoch: 1 [1710/1165029 (0%)]\tLoss: 7.209623\n",
      "2024-07-27 23:46:39 - __main__ - INFO - Train Epoch: 1 [1720/1165029 (0%)]\tLoss: 7.571253\n",
      "2024-07-27 23:46:39 - __main__ - INFO - Train Epoch: 1 [1730/1165029 (0%)]\tLoss: 7.485439\n",
      "2024-07-27 23:46:39 - __main__ - INFO - Train Epoch: 1 [1740/1165029 (0%)]\tLoss: 6.275785\n",
      "2024-07-27 23:46:39 - __main__ - INFO - Train Epoch: 1 [1750/1165029 (0%)]\tLoss: 7.723782\n",
      "2024-07-27 23:46:39 - __main__ - INFO - Train Epoch: 1 [1760/1165029 (0%)]\tLoss: 7.064833\n",
      "2024-07-27 23:46:39 - __main__ - INFO - Train Epoch: 1 [1770/1165029 (0%)]\tLoss: 3.905460\n",
      "2024-07-27 23:46:39 - __main__ - INFO - Train Epoch: 1 [1780/1165029 (0%)]\tLoss: 7.085481\n",
      "2024-07-27 23:46:40 - __main__ - INFO - Train Epoch: 1 [1790/1165029 (0%)]\tLoss: 7.551980\n",
      "2024-07-27 23:46:40 - __main__ - INFO - Train Epoch: 1 [1800/1165029 (0%)]\tLoss: 7.236485\n",
      "2024-07-27 23:46:40 - __main__ - INFO - Train Epoch: 1 [1810/1165029 (0%)]\tLoss: 7.061795\n",
      "2024-07-27 23:46:40 - __main__ - INFO - Train Epoch: 1 [1820/1165029 (0%)]\tLoss: 7.730336\n",
      "2024-07-27 23:46:40 - __main__ - INFO - Train Epoch: 1 [1830/1165029 (0%)]\tLoss: 3.524109\n",
      "2024-07-27 23:46:40 - __main__ - INFO - Train Epoch: 1 [1840/1165029 (0%)]\tLoss: 7.598545\n",
      "2024-07-27 23:46:40 - __main__ - INFO - Train Epoch: 1 [1850/1165029 (0%)]\tLoss: 7.667178\n",
      "2024-07-27 23:46:40 - __main__ - INFO - Train Epoch: 1 [1860/1165029 (0%)]\tLoss: 3.628509\n",
      "2024-07-27 23:46:40 - __main__ - INFO - Train Epoch: 1 [1870/1165029 (0%)]\tLoss: 2.343812\n",
      "2024-07-27 23:46:40 - __main__ - INFO - Train Epoch: 1 [1880/1165029 (0%)]\tLoss: 7.363172\n",
      "2024-07-27 23:46:41 - __main__ - INFO - Train Epoch: 1 [1890/1165029 (0%)]\tLoss: 6.996775\n",
      "2024-07-27 23:46:41 - __main__ - INFO - Train Epoch: 1 [1900/1165029 (0%)]\tLoss: 4.422557\n",
      "2024-07-27 23:46:41 - __main__ - INFO - Train Epoch: 1 [1910/1165029 (0%)]\tLoss: 6.021594\n",
      "2024-07-27 23:46:41 - __main__ - INFO - Train Epoch: 1 [1920/1165029 (0%)]\tLoss: 7.474873\n",
      "2024-07-27 23:46:41 - __main__ - INFO - Train Epoch: 1 [1930/1165029 (0%)]\tLoss: 3.752532\n",
      "2024-07-27 23:46:41 - __main__ - INFO - Train Epoch: 1 [1940/1165029 (0%)]\tLoss: 7.266965\n",
      "2024-07-27 23:46:41 - __main__ - INFO - Train Epoch: 1 [1950/1165029 (0%)]\tLoss: 7.265121\n",
      "2024-07-27 23:46:41 - __main__ - INFO - Train Epoch: 1 [1960/1165029 (0%)]\tLoss: 7.515376\n",
      "2024-07-27 23:46:42 - __main__ - INFO - Train Epoch: 1 [1970/1165029 (0%)]\tLoss: 7.464761\n",
      "2024-07-27 23:46:42 - __main__ - INFO - Train Epoch: 1 [1980/1165029 (0%)]\tLoss: 8.295096\n",
      "2024-07-27 23:46:42 - __main__ - INFO - Train Epoch: 1 [1990/1165029 (0%)]\tLoss: 7.265890\n",
      "2024-07-27 23:46:42 - __main__ - INFO - Train Epoch: 1 [2000/1165029 (0%)]\tLoss: 7.237828\n",
      "2024-07-27 23:46:42 - __main__ - INFO - Train Epoch: 1 [2010/1165029 (0%)]\tLoss: 7.069316\n",
      "2024-07-27 23:46:42 - __main__ - INFO - Train Epoch: 1 [2020/1165029 (0%)]\tLoss: 7.366809\n",
      "2024-07-27 23:46:42 - __main__ - INFO - Train Epoch: 1 [2030/1165029 (0%)]\tLoss: 7.400204\n",
      "2024-07-27 23:46:42 - __main__ - INFO - Train Epoch: 1 [2040/1165029 (0%)]\tLoss: 7.150292\n",
      "2024-07-27 23:46:42 - __main__ - INFO - Train Epoch: 1 [2050/1165029 (0%)]\tLoss: 7.010043\n",
      "2024-07-27 23:46:43 - __main__ - INFO - Train Epoch: 1 [2060/1165029 (0%)]\tLoss: 7.674466\n",
      "2024-07-27 23:46:43 - __main__ - INFO - Train Epoch: 1 [2070/1165029 (0%)]\tLoss: 7.233675\n",
      "2024-07-27 23:46:43 - __main__ - INFO - Train Epoch: 1 [2080/1165029 (0%)]\tLoss: 7.863487\n",
      "2024-07-27 23:46:43 - __main__ - INFO - Train Epoch: 1 [2090/1165029 (0%)]\tLoss: 7.190488\n",
      "2024-07-27 23:46:43 - __main__ - INFO - Train Epoch: 1 [2100/1165029 (0%)]\tLoss: 4.179462\n",
      "2024-07-27 23:46:43 - __main__ - INFO - Train Epoch: 1 [2110/1165029 (0%)]\tLoss: 5.557271\n",
      "2024-07-27 23:46:43 - __main__ - INFO - Train Epoch: 1 [2120/1165029 (0%)]\tLoss: 8.037410\n",
      "2024-07-27 23:46:43 - __main__ - INFO - Train Epoch: 1 [2130/1165029 (0%)]\tLoss: 7.409037\n",
      "2024-07-27 23:46:43 - __main__ - INFO - Train Epoch: 1 [2140/1165029 (0%)]\tLoss: 7.380476\n",
      "2024-07-27 23:46:44 - __main__ - INFO - Train Epoch: 1 [2150/1165029 (0%)]\tLoss: 7.250233\n",
      "2024-07-27 23:46:44 - __main__ - INFO - Train Epoch: 1 [2160/1165029 (0%)]\tLoss: 5.713644\n",
      "2024-07-27 23:46:44 - __main__ - INFO - Train Epoch: 1 [2170/1165029 (0%)]\tLoss: 7.676630\n",
      "2024-07-27 23:46:44 - __main__ - INFO - Train Epoch: 1 [2180/1165029 (0%)]\tLoss: 7.172451\n",
      "2024-07-27 23:46:44 - __main__ - INFO - Train Epoch: 1 [2190/1165029 (0%)]\tLoss: 7.323399\n",
      "2024-07-27 23:46:44 - __main__ - INFO - Train Epoch: 1 [2200/1165029 (0%)]\tLoss: 7.000667\n",
      "2024-07-27 23:46:44 - __main__ - INFO - Train Epoch: 1 [2210/1165029 (0%)]\tLoss: 6.318251\n",
      "2024-07-27 23:46:44 - __main__ - INFO - Train Epoch: 1 [2220/1165029 (0%)]\tLoss: 2.279836\n",
      "2024-07-27 23:46:44 - __main__ - INFO - Train Epoch: 1 [2230/1165029 (0%)]\tLoss: 7.162837\n",
      "2024-07-27 23:46:45 - __main__ - INFO - Train Epoch: 1 [2240/1165029 (0%)]\tLoss: 7.192237\n",
      "2024-07-27 23:46:45 - __main__ - INFO - Train Epoch: 1 [2250/1165029 (0%)]\tLoss: 7.356294\n",
      "2024-07-27 23:46:45 - __main__ - INFO - Train Epoch: 1 [2260/1165029 (0%)]\tLoss: 7.632599\n",
      "2024-07-27 23:46:45 - __main__ - INFO - Train Epoch: 1 [2270/1165029 (0%)]\tLoss: 7.272158\n",
      "2024-07-27 23:46:45 - __main__ - INFO - Train Epoch: 1 [2280/1165029 (0%)]\tLoss: 7.044791\n",
      "2024-07-27 23:46:45 - __main__ - INFO - Train Epoch: 1 [2290/1165029 (0%)]\tLoss: 7.508652\n",
      "2024-07-27 23:46:45 - __main__ - INFO - Train Epoch: 1 [2300/1165029 (0%)]\tLoss: 7.356527\n",
      "2024-07-27 23:46:45 - __main__ - INFO - Train Epoch: 1 [2310/1165029 (0%)]\tLoss: 7.070859\n",
      "2024-07-27 23:46:45 - __main__ - INFO - Train Epoch: 1 [2320/1165029 (0%)]\tLoss: 2.107006\n",
      "2024-07-27 23:46:45 - __main__ - INFO - Train Epoch: 1 [2330/1165029 (0%)]\tLoss: 7.131431\n",
      "2024-07-27 23:46:46 - __main__ - INFO - Train Epoch: 1 [2340/1165029 (0%)]\tLoss: 1.767223\n",
      "2024-07-27 23:46:46 - __main__ - INFO - Train Epoch: 1 [2350/1165029 (0%)]\tLoss: 7.702064\n",
      "2024-07-27 23:46:46 - __main__ - INFO - Train Epoch: 1 [2360/1165029 (0%)]\tLoss: 7.080538\n",
      "2024-07-27 23:46:46 - __main__ - INFO - Train Epoch: 1 [2370/1165029 (0%)]\tLoss: 7.197867\n",
      "2024-07-27 23:46:46 - __main__ - INFO - Train Epoch: 1 [2380/1165029 (0%)]\tLoss: 7.411445\n",
      "2024-07-27 23:46:46 - __main__ - INFO - Train Epoch: 1 [2390/1165029 (0%)]\tLoss: 3.094711\n",
      "2024-07-27 23:46:46 - __main__ - INFO - Train Epoch: 1 [2400/1165029 (0%)]\tLoss: 7.507007\n",
      "2024-07-27 23:46:46 - __main__ - INFO - Train Epoch: 1 [2410/1165029 (0%)]\tLoss: 5.535058\n",
      "2024-07-27 23:46:46 - __main__ - INFO - Train Epoch: 1 [2420/1165029 (0%)]\tLoss: 7.478443\n",
      "2024-07-27 23:46:47 - __main__ - INFO - Train Epoch: 1 [2430/1165029 (0%)]\tLoss: 7.240111\n",
      "2024-07-27 23:46:47 - __main__ - INFO - Train Epoch: 1 [2440/1165029 (0%)]\tLoss: 7.934225\n",
      "2024-07-27 23:46:47 - __main__ - INFO - Train Epoch: 1 [2450/1165029 (0%)]\tLoss: 7.444194\n",
      "2024-07-27 23:46:47 - __main__ - INFO - Train Epoch: 1 [2460/1165029 (0%)]\tLoss: 7.124782\n",
      "2024-07-27 23:46:47 - __main__ - INFO - Train Epoch: 1 [2470/1165029 (0%)]\tLoss: 7.348118\n",
      "2024-07-27 23:46:47 - __main__ - INFO - Train Epoch: 1 [2480/1165029 (0%)]\tLoss: 7.146925\n",
      "2024-07-27 23:46:47 - __main__ - INFO - Train Epoch: 1 [2490/1165029 (0%)]\tLoss: 7.427296\n",
      "2024-07-27 23:46:47 - __main__ - INFO - Train Epoch: 1 [2500/1165029 (0%)]\tLoss: 4.882966\n",
      "2024-07-27 23:46:47 - __main__ - INFO - Train Epoch: 1 [2510/1165029 (0%)]\tLoss: 7.153176\n",
      "2024-07-27 23:46:47 - __main__ - INFO - Train Epoch: 1 [2520/1165029 (0%)]\tLoss: 7.326176\n",
      "2024-07-27 23:46:48 - __main__ - INFO - Train Epoch: 1 [2530/1165029 (0%)]\tLoss: 5.897746\n",
      "2024-07-27 23:46:48 - __main__ - INFO - Train Epoch: 1 [2540/1165029 (0%)]\tLoss: 7.459827\n",
      "2024-07-27 23:46:48 - __main__ - INFO - Train Epoch: 1 [2550/1165029 (0%)]\tLoss: 7.365308\n",
      "2024-07-27 23:46:48 - __main__ - INFO - Train Epoch: 1 [2560/1165029 (0%)]\tLoss: 7.238855\n",
      "2024-07-27 23:46:48 - __main__ - INFO - Train Epoch: 1 [2570/1165029 (0%)]\tLoss: 2.877792\n",
      "2024-07-27 23:46:48 - __main__ - INFO - Train Epoch: 1 [2580/1165029 (0%)]\tLoss: 7.410168\n",
      "2024-07-27 23:46:48 - __main__ - INFO - Train Epoch: 1 [2590/1165029 (0%)]\tLoss: 2.583894\n",
      "2024-07-27 23:46:48 - __main__ - INFO - Train Epoch: 1 [2600/1165029 (0%)]\tLoss: 6.976621\n",
      "2024-07-27 23:46:48 - __main__ - INFO - Train Epoch: 1 [2610/1165029 (0%)]\tLoss: 7.495468\n",
      "2024-07-27 23:46:49 - __main__ - INFO - Train Epoch: 1 [2620/1165029 (0%)]\tLoss: 8.291754\n",
      "2024-07-27 23:46:49 - __main__ - INFO - Train Epoch: 1 [2630/1165029 (0%)]\tLoss: 7.187623\n",
      "2024-07-27 23:46:49 - __main__ - INFO - Train Epoch: 1 [2640/1165029 (0%)]\tLoss: 5.553456\n",
      "2024-07-27 23:46:49 - __main__ - INFO - Train Epoch: 1 [2650/1165029 (0%)]\tLoss: 7.804843\n",
      "2024-07-27 23:46:49 - __main__ - INFO - Train Epoch: 1 [2660/1165029 (0%)]\tLoss: 7.579232\n",
      "2024-07-27 23:46:49 - __main__ - INFO - Train Epoch: 1 [2670/1165029 (0%)]\tLoss: 3.489638\n",
      "2024-07-27 23:46:49 - __main__ - INFO - Train Epoch: 1 [2680/1165029 (0%)]\tLoss: 7.414571\n",
      "2024-07-27 23:46:49 - __main__ - INFO - Train Epoch: 1 [2690/1165029 (0%)]\tLoss: 7.104590\n",
      "2024-07-27 23:46:49 - __main__ - INFO - Train Epoch: 1 [2700/1165029 (0%)]\tLoss: 7.379184\n",
      "2024-07-27 23:46:50 - __main__ - INFO - Train Epoch: 1 [2710/1165029 (0%)]\tLoss: 2.657183\n",
      "2024-07-27 23:46:50 - __main__ - INFO - Train Epoch: 1 [2720/1165029 (0%)]\tLoss: 7.677746\n",
      "2024-07-27 23:46:50 - __main__ - INFO - Train Epoch: 1 [2730/1165029 (0%)]\tLoss: 7.505558\n",
      "2024-07-27 23:46:50 - __main__ - INFO - Train Epoch: 1 [2740/1165029 (0%)]\tLoss: 7.194152\n",
      "2024-07-27 23:46:50 - __main__ - INFO - Train Epoch: 1 [2750/1165029 (0%)]\tLoss: 7.255049\n",
      "2024-07-27 23:46:50 - __main__ - INFO - Train Epoch: 1 [2760/1165029 (0%)]\tLoss: 7.569392\n",
      "2024-07-27 23:46:50 - __main__ - INFO - Train Epoch: 1 [2770/1165029 (0%)]\tLoss: 4.362887\n",
      "2024-07-27 23:46:50 - __main__ - INFO - Train Epoch: 1 [2780/1165029 (0%)]\tLoss: 7.379838\n",
      "2024-07-27 23:46:50 - __main__ - INFO - Train Epoch: 1 [2790/1165029 (0%)]\tLoss: 7.242444\n",
      "2024-07-27 23:46:51 - __main__ - INFO - Train Epoch: 1 [2800/1165029 (0%)]\tLoss: 7.301971\n",
      "2024-07-27 23:46:51 - __main__ - INFO - Train Epoch: 1 [2810/1165029 (0%)]\tLoss: 7.481524\n",
      "2024-07-27 23:46:51 - __main__ - INFO - Train Epoch: 1 [2820/1165029 (0%)]\tLoss: 3.405244\n",
      "2024-07-27 23:46:51 - __main__ - INFO - Train Epoch: 1 [2830/1165029 (0%)]\tLoss: 7.281169\n",
      "2024-07-27 23:46:51 - __main__ - INFO - Train Epoch: 1 [2840/1165029 (0%)]\tLoss: 7.293468\n",
      "2024-07-27 23:46:51 - __main__ - INFO - Train Epoch: 1 [2850/1165029 (0%)]\tLoss: 4.416183\n",
      "2024-07-27 23:46:51 - __main__ - INFO - Train Epoch: 1 [2860/1165029 (0%)]\tLoss: 7.484262\n",
      "2024-07-27 23:46:51 - __main__ - INFO - Train Epoch: 1 [2870/1165029 (0%)]\tLoss: 6.629780\n",
      "2024-07-27 23:46:51 - __main__ - INFO - Train Epoch: 1 [2880/1165029 (0%)]\tLoss: 7.230855\n",
      "2024-07-27 23:46:52 - __main__ - INFO - Train Epoch: 1 [2890/1165029 (0%)]\tLoss: 7.143212\n",
      "2024-07-27 23:46:52 - __main__ - INFO - Train Epoch: 1 [2900/1165029 (0%)]\tLoss: 7.496478\n",
      "2024-07-27 23:46:52 - __main__ - INFO - Train Epoch: 1 [2910/1165029 (0%)]\tLoss: 7.545720\n",
      "2024-07-27 23:46:52 - __main__ - INFO - Train Epoch: 1 [2920/1165029 (0%)]\tLoss: 7.138857\n",
      "2024-07-27 23:46:52 - __main__ - INFO - Train Epoch: 1 [2930/1165029 (0%)]\tLoss: 7.179303\n",
      "2024-07-27 23:46:52 - __main__ - INFO - Train Epoch: 1 [2940/1165029 (0%)]\tLoss: 7.216469\n",
      "2024-07-27 23:46:52 - __main__ - INFO - Train Epoch: 1 [2950/1165029 (0%)]\tLoss: 7.833807\n",
      "2024-07-27 23:46:52 - __main__ - INFO - Train Epoch: 1 [2960/1165029 (0%)]\tLoss: 7.231895\n",
      "2024-07-27 23:46:52 - __main__ - INFO - Train Epoch: 1 [2970/1165029 (0%)]\tLoss: 7.375589\n",
      "2024-07-27 23:46:53 - __main__ - INFO - Train Epoch: 1 [2980/1165029 (0%)]\tLoss: 3.290066\n",
      "2024-07-27 23:46:53 - __main__ - INFO - Train Epoch: 1 [2990/1165029 (0%)]\tLoss: 7.478351\n",
      "2024-07-27 23:46:53 - __main__ - INFO - Train Epoch: 1 [3000/1165029 (0%)]\tLoss: 7.348908\n",
      "2024-07-27 23:46:53 - __main__ - INFO - Train Epoch: 1 [3010/1165029 (0%)]\tLoss: 7.225636\n",
      "2024-07-27 23:46:53 - __main__ - INFO - Train Epoch: 1 [3020/1165029 (0%)]\tLoss: 7.590552\n",
      "2024-07-27 23:46:53 - __main__ - INFO - Train Epoch: 1 [3030/1165029 (0%)]\tLoss: 7.758188\n",
      "2024-07-27 23:46:53 - __main__ - INFO - Train Epoch: 1 [3040/1165029 (0%)]\tLoss: 7.178782\n",
      "2024-07-27 23:46:53 - __main__ - INFO - Train Epoch: 1 [3050/1165029 (0%)]\tLoss: 7.499354\n",
      "2024-07-27 23:46:54 - __main__ - INFO - Train Epoch: 1 [3060/1165029 (0%)]\tLoss: 6.854400\n",
      "2024-07-27 23:46:54 - __main__ - INFO - Train Epoch: 1 [3070/1165029 (0%)]\tLoss: 6.747263\n",
      "2024-07-27 23:46:54 - __main__ - INFO - Train Epoch: 1 [3080/1165029 (0%)]\tLoss: 5.562528\n",
      "2024-07-27 23:46:54 - __main__ - INFO - Train Epoch: 1 [3090/1165029 (0%)]\tLoss: 3.363081\n",
      "2024-07-27 23:46:54 - __main__ - INFO - Train Epoch: 1 [3100/1165029 (0%)]\tLoss: 5.151505\n",
      "2024-07-27 23:46:54 - __main__ - INFO - Train Epoch: 1 [3110/1165029 (0%)]\tLoss: 3.562446\n",
      "2024-07-27 23:46:54 - __main__ - INFO - Train Epoch: 1 [3120/1165029 (0%)]\tLoss: 7.035809\n",
      "2024-07-27 23:46:54 - __main__ - INFO - Train Epoch: 1 [3130/1165029 (0%)]\tLoss: 7.127489\n",
      "2024-07-27 23:46:54 - __main__ - INFO - Train Epoch: 1 [3140/1165029 (0%)]\tLoss: 7.290882\n",
      "2024-07-27 23:46:55 - __main__ - INFO - Train Epoch: 1 [3150/1165029 (0%)]\tLoss: 7.653955\n",
      "2024-07-27 23:46:55 - __main__ - INFO - Train Epoch: 1 [3160/1165029 (0%)]\tLoss: 7.290685\n",
      "2024-07-27 23:46:55 - __main__ - INFO - Train Epoch: 1 [3170/1165029 (0%)]\tLoss: 7.165466\n",
      "2024-07-27 23:46:55 - __main__ - INFO - Train Epoch: 1 [3180/1165029 (0%)]\tLoss: 3.305987\n",
      "2024-07-27 23:46:55 - __main__ - INFO - Train Epoch: 1 [3190/1165029 (0%)]\tLoss: 7.198725\n",
      "2024-07-27 23:46:55 - __main__ - INFO - Train Epoch: 1 [3200/1165029 (0%)]\tLoss: 7.275942\n",
      "2024-07-27 23:46:55 - __main__ - INFO - Train Epoch: 1 [3210/1165029 (0%)]\tLoss: 6.959155\n",
      "2024-07-27 23:46:55 - __main__ - INFO - Train Epoch: 1 [3220/1165029 (0%)]\tLoss: 4.995357\n",
      "2024-07-27 23:46:55 - __main__ - INFO - Train Epoch: 1 [3230/1165029 (0%)]\tLoss: 4.208473\n",
      "2024-07-27 23:46:55 - __main__ - INFO - Train Epoch: 1 [3240/1165029 (0%)]\tLoss: 6.976175\n",
      "2024-07-27 23:46:56 - __main__ - INFO - Train Epoch: 1 [3250/1165029 (0%)]\tLoss: 7.210610\n",
      "2024-07-27 23:46:56 - __main__ - INFO - Train Epoch: 1 [3260/1165029 (0%)]\tLoss: 3.046331\n",
      "2024-07-27 23:46:56 - __main__ - INFO - Train Epoch: 1 [3270/1165029 (0%)]\tLoss: 7.540821\n",
      "2024-07-27 23:46:56 - __main__ - INFO - Train Epoch: 1 [3280/1165029 (0%)]\tLoss: 6.931962\n",
      "2024-07-27 23:46:56 - __main__ - INFO - Train Epoch: 1 [3290/1165029 (0%)]\tLoss: 5.116707\n",
      "2024-07-27 23:46:56 - __main__ - INFO - Train Epoch: 1 [3300/1165029 (0%)]\tLoss: 7.229701\n",
      "2024-07-27 23:46:56 - __main__ - INFO - Train Epoch: 1 [3310/1165029 (0%)]\tLoss: 3.426039\n",
      "2024-07-27 23:46:56 - __main__ - INFO - Train Epoch: 1 [3320/1165029 (0%)]\tLoss: 7.843802\n",
      "2024-07-27 23:46:56 - __main__ - INFO - Train Epoch: 1 [3330/1165029 (0%)]\tLoss: 7.094154\n",
      "2024-07-27 23:46:57 - __main__ - INFO - Train Epoch: 1 [3340/1165029 (0%)]\tLoss: 7.423786\n",
      "2024-07-27 23:46:57 - __main__ - INFO - Train Epoch: 1 [3350/1165029 (0%)]\tLoss: 3.727443\n",
      "2024-07-27 23:46:57 - __main__ - INFO - Train Epoch: 1 [3360/1165029 (0%)]\tLoss: 7.509286\n",
      "2024-07-27 23:46:57 - __main__ - INFO - Train Epoch: 1 [3370/1165029 (0%)]\tLoss: 7.242899\n",
      "2024-07-27 23:46:57 - __main__ - INFO - Train Epoch: 1 [3380/1165029 (0%)]\tLoss: 7.107385\n",
      "2024-07-27 23:46:57 - __main__ - INFO - Train Epoch: 1 [3390/1165029 (0%)]\tLoss: 7.397176\n",
      "2024-07-27 23:46:57 - __main__ - INFO - Train Epoch: 1 [3400/1165029 (0%)]\tLoss: 4.281325\n",
      "2024-07-27 23:46:57 - __main__ - INFO - Train Epoch: 1 [3410/1165029 (0%)]\tLoss: 7.210592\n",
      "2024-07-27 23:46:57 - __main__ - INFO - Train Epoch: 1 [3420/1165029 (0%)]\tLoss: 7.352662\n",
      "2024-07-27 23:46:57 - __main__ - INFO - Train Epoch: 1 [3430/1165029 (0%)]\tLoss: 7.502356\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 100\u001b[0m\n\u001b[0;32m     97\u001b[0m     train, evaluate \u001b[38;5;241m=\u001b[39m pretrain_train, pretrain_evaluate\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m--> 100\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_wandb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m evaluate(model, test_dataloader, criterion, device, logger, args\u001b[38;5;241m.\u001b[39muse_wandb)\n\u001b[0;32m    102\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m, in \u001b[0;36mpretrain_train\u001b[1;34m(model, dataloader, criterion, optimizer, device, epoch, logger, use_wandb)\u001b[0m\n\u001b[0;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_texts)\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), target_texts\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     20\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32md:\\Studies\\RajaCourse\\hw1\\.venv\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Studies\\RajaCourse\\hw1\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Studies\\RajaCourse\\hw1\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "\n",
    "\n",
    "model_options = list(config[\"models\"].keys())\n",
    "run_types = [\"task\", \"lra_pretrain\", \"wikitext_pretrain\", \"task_finetune_lra_pretrain\", \"task_finetune_wikitext_pretrain\"]\n",
    "class Args:\n",
    "    def __init__(self, model, run_type, use_wandb = False):\n",
    "        self.model = model\n",
    "        self.run_type = run_type\n",
    "        self.use_wandb = use_wandb\n",
    "\n",
    "args = Args(model=\"transformer\", run_type=\"wikitext_pretrain\", use_wandb=False)\n",
    "\n",
    "model_config = config[\"models\"][args.model]\n",
    "run_parameters = config[\"run_parameters\"]\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "run_name = f\"run_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{args.model}_{args.run_type}\"\n",
    "\n",
    "batch_size, n_epochs, learning_rate = run_parameters[\"batch_size\"], run_parameters[\"n_epochs\"], run_parameters[\n",
    "    \"learning_rate\"]\n",
    "\n",
    "# Load tokenizer and data loaders\n",
    "if \"wikitext\" in args.run_type and \"finetune\" in args.run_type:\n",
    "    get_tokenizer_and_vocab = WikiTextDataset.get_tokenizer_and_vocab\n",
    "    get_dataloaders = IMDBDataset.get_dataloaders\n",
    "elif \"wikitext\" in args.run_type:\n",
    "    get_tokenizer_and_vocab = WikiTextDataset.get_tokenizer_and_vocab\n",
    "    get_dataloaders = WikiTextDataset.get_dataloaders\n",
    "else:\n",
    "    get_tokenizer_and_vocab = IMDBDataset.get_tokenizer_and_vocab\n",
    "    get_dataloaders = IMDBDataset.get_dataloaders\n",
    "tokenizer = get_tokenizer_and_vocab()\n",
    "train_dataloader, test_dataloader = get_dataloaders(batch_size)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "finetune = args.run_type in [\"task\", \"task_finetune_lra_pretrain\", \"task_finetune_wikitext_pretrain\"]\n",
    "\n",
    "model_dic = {\n",
    "    \"lstm\": CustomLSTMModel,\n",
    "    \"s4\": S4Model,\n",
    "    \"transformer\": CustomTransformerModel\n",
    "}\n",
    "\n",
    "if args.run_type == \"task\":\n",
    "    config[\"models\"][args.model][\"output_dim\"] = 1\n",
    "else:\n",
    "    config[\"models\"][args.model][\"output_dim\"] = vocab_size\n",
    "\n",
    "model = model_dic[args.model](vocab_size=vocab_size, **config[\"models\"][args.model], finetune=finetune)\n",
    "model.to(device)\n",
    "\n",
    "pretrained = \"lra_pretrained\" if \"lra\" in args.run_type else \"wikitext_pretrained\"\n",
    "checkpoint_path = f\"{args.model}_{pretrained}.pth\"\n",
    "if finetune and args.run_type != \"task\":\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    logger.info(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "    replace_final_layer(model, config, args.model, device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss() if finetune else torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "logger.info(f\"parameter_cnt: {model.count_parameters}\")\n",
    "\n",
    "if args.use_wandb:\n",
    "    # Initialize WandB\n",
    "    wandb.login(key=\"5fda0926085bc8963be5e43c4e501d992e35abe8\")\n",
    "    wandb.init(project=\"model-comparison\", name=run_name)\n",
    "\n",
    "    # Log hyperparameters and model\n",
    "    wandb.config.update(\n",
    "        {\n",
    "            **{\n",
    "                \"run_name\": run_name,\n",
    "                \"model\": args.model,\n",
    "                \"parameter_cnt\": model.count_parameters,\n",
    "            },\n",
    "            **config[\"models\"][args.model],\n",
    "            **config[\"run_parameters\"]}\n",
    "    )\n",
    "\n",
    "# Training loop\n",
    "logger.info(\"Starting training...\")\n",
    "\n",
    "if finetune:\n",
    "    train, evaluate = finetune_train, finetune_evaluate\n",
    "else:\n",
    "    train, evaluate = pretrain_train, pretrain_evaluate\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer, device, epoch, logger, args.use_wandb)\n",
    "    test_loss = evaluate(model, test_dataloader, criterion, device, logger, args.use_wandb)\n",
    "    logger.info(f\"Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    if args.use_wandb:\n",
    "        # Log metrics to WandB\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss\n",
    "        })\n",
    "\n",
    "pretrained = \"lra_pretrained\" if \"lra\" in args.run_type else \"wikitext_pretrained\"\n",
    "checkpoint_path = f\"{args.model}_{pretrained}.pth\"\n",
    "if not finetune:\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    logger.info(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "logger.info(\"Training completed.\")\n",
    "\n",
    "if args.use_wandb:\n",
    "    wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
