{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "8a71iASq9Gja",
    "outputId": "58d77fe1-cbfe-4394-9491-f5909fc998c2",
    "ExecuteTime": {
     "end_time": "2024-08-03T13:39:26.334363Z",
     "start_time": "2024-08-03T13:39:26.281964Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ZmMhUpmj_Jpd",
    "outputId": "ee0ea313-2612-4927-9eaa-dc7f883b188e",
    "ExecuteTime": {
     "end_time": "2024-08-03T13:39:26.355103Z",
     "start_time": "2024-08-03T13:39:26.332299Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip3 install datasets wandb einops torchdata==0.7.1 portalocker==2.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdhbNNb-SCAG"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7tFZLjwMSCAH",
    "outputId": "25250b5c-773c-43ff-ec44-dc4d2dc42df6",
    "ExecuteTime": {
     "end_time": "2024-08-03T13:39:26.372003Z",
     "start_time": "2024-08-03T13:39:26.351186Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import datetime\n",
    "import random\n",
    "import logging\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import wandb\n",
    "from functools import cached_property\n",
    "from einops import rearrange, repeat\n",
    "from datasets import load_dataset\n",
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.datasets import IMDB\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJYGyX9Yk4eo"
   },
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCQ3ZAGvk-MD"
   },
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "5AlllYobk6nj",
    "ExecuteTime": {
     "end_time": "2024-08-03T13:39:26.388663Z",
     "start_time": "2024-08-03T13:39:26.378217Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, output_dim, finetune=True):\n",
    "        super(CustomLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.finetune = finetune\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Create layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            input_dim = embed_dim if i == 0 else hidden_dim\n",
    "            self.layers.append(LSTMLayer(input_dim, hidden_dim))\n",
    "\n",
    "        # Output layer weights and biases\n",
    "        self.Wy = nn.Parameter(torch.empty(output_dim, hidden_dim))\n",
    "        self.by = nn.Parameter(torch.zeros(output_dim, 1))\n",
    "\n",
    "        # Initialize weights using Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.Wy)\n",
    "\n",
    "    def forward(self, texts):\n",
    "        batch_size, seq_len = texts.size()\n",
    "        embedded = self.embedding(texts).permute(1, 2, 0)  # (seq_len, embed_dim, batch_size)\n",
    "\n",
    "        h = [\n",
    "            torch.zeros(self.hidden_dim, batch_size).to(texts.device)\n",
    "            for _ in range(self.num_layers)\n",
    "        ]\n",
    "        c = [\n",
    "            torch.zeros(self.hidden_dim, batch_size).to(texts.device)\n",
    "            for _ in range(self.num_layers)\n",
    "        ]\n",
    "\n",
    "        if not self.finetune:\n",
    "            y = torch.zeros(seq_len, batch_size, self.vocab_size).to(texts.device)  # (seq_len, batch_size, vocab_size)\n",
    "            for t in range(seq_len):\n",
    "                x = embedded[t, :, :]  # (embed_dim, batch_size)\n",
    "                for i, layer in enumerate(self.layers):\n",
    "                    h[i], c[i] = layer(x, h[i], c[i])\n",
    "                    x = h[i]\n",
    "                h_last = h[-1].permute(1, 0)  # (batch_size, hidden_dim)\n",
    "                y[t] = torch.matmul(h_last, self.Wy.t()) + self.by.t()  # (batch_size, vocab_size)\n",
    "\n",
    "            return y.permute(1, 0, 2)\n",
    "        else:\n",
    "            for t in range(seq_len):\n",
    "                x = embedded[t, :, :]  # (embed_dim, batch_size)\n",
    "                for i, layer in enumerate(self.layers):\n",
    "                    h[i], c[i] = layer(x, h[i], c[i])\n",
    "                    x = h[i]\n",
    "\n",
    "            h_last = h[-1].permute(1, 0)  # (batch_size, hidden_dim)\n",
    "            y = torch.matmul(h_last, self.Wy.t()) + self.by.t()  # (batch_size, output_dim)\n",
    "            return y.squeeze(1)\n",
    "\n",
    "    @cached_property\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # LSTM weights and biases\n",
    "        self.Wf = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
    "        self.bf = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
    "        self.Wi = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
    "        self.bi = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
    "        self.Wo = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
    "        self.bo = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
    "        self.Wc = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
    "        self.bc = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
    "\n",
    "        # Initialize weights using Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.Wf)\n",
    "        nn.init.xavier_uniform_(self.Wi)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.Wc)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return torch.tanh(x)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        concat = torch.cat((h, x), dim=0)  # (hidden_dim + input_dim, batch_size)\n",
    "\n",
    "        ft = self.sigmoid(\n",
    "            torch.matmul(self.Wf, concat) + self.bf\n",
    "        )  # (hidden, batch_size)\n",
    "        it = self.sigmoid(\n",
    "            torch.matmul(self.Wi, concat) + self.bi\n",
    "        )  # (hidden, batch_size)\n",
    "        c_hat = self.tanh(\n",
    "            torch.matmul(self.Wc, concat) + self.bc\n",
    "        )  # (hidden, batch_size)\n",
    "        c = ft * c + it * c_hat  # (hidden, batch_size)\n",
    "        ot = self.sigmoid(\n",
    "            torch.matmul(self.Wo, concat) + self.bo\n",
    "        )  # (hidden, batch_size)\n",
    "        h = ot * self.tanh(c)\n",
    "\n",
    "        return h, c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwSlNHbilAPX"
   },
   "source": [
    "S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "zaiAg-gmlBVS",
    "ExecuteTime": {
     "end_time": "2024-08-03T13:39:26.411677Z",
     "start_time": "2024-08-03T13:39:26.394984Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DropoutNd(nn.Module):\n",
    "    def __init__(self, p: float = 0.5, tie=True, transposed=True):\n",
    "        \"\"\"\n",
    "        tie: tie dropout mask across sequence lengths (Dropout1d/2d/3d)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if p < 0 or p >= 1:\n",
    "            raise ValueError(\"dropout probability has to be in [0, 1), \" \"but got {}\".format(p))\n",
    "        self.p = p\n",
    "        self.tie = tie\n",
    "        self.transposed = transposed\n",
    "        self.binomial = torch.distributions.binomial.Binomial(probs=1 - self.p)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"X: (batch, dim, lengths...).\"\"\"\n",
    "        if self.training:\n",
    "            if not self.transposed: X = rearrange(X, 'b ... d -> b d ...')\n",
    "            # binomial = torch.distributions.binomial.Binomial(probs=1-self.p) # This is incredibly slow because of CPU -> GPU copying\n",
    "            mask_shape = X.shape[:2] + (1,) * (X.ndim - 2) if self.tie else X.shape\n",
    "            # mask = self.binomial.sample(mask_shape)\n",
    "            mask = torch.rand(*mask_shape, device=X.device) < 1. - self.p\n",
    "            X = X * mask * (1.0 / (1 - self.p))\n",
    "            if not self.transposed: X = rearrange(X, 'b d ... -> b ... d')\n",
    "            return X\n",
    "        return X\n",
    "\n",
    "\n",
    "class S4DKernel(nn.Module):\n",
    "    \"\"\"Generate convolution kernel from diagonal SSM parameters.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, N=64, dt_min=0.001, dt_max=0.1, lr=None):\n",
    "        super().__init__()\n",
    "        # Generate dt\n",
    "        H = hidden_dim\n",
    "        log_dt = torch.rand(H) * (\n",
    "                math.log(dt_max) - math.log(dt_min)\n",
    "        ) + math.log(dt_min)\n",
    "\n",
    "        C = torch.randn(H, N // 2, dtype=torch.cfloat)\n",
    "        self.C = nn.Parameter(torch.view_as_real(C))\n",
    "        self.register(\"log_dt\", log_dt, lr)\n",
    "\n",
    "        log_A_real = torch.log(0.5 * torch.ones(H, N // 2))\n",
    "        A_imag = math.pi * repeat(torch.arange(N // 2), 'n -> h n', h=H)\n",
    "        self.register(\"log_A_real\", log_A_real, lr)\n",
    "        self.register(\"A_imag\", A_imag, lr)\n",
    "\n",
    "    def forward(self, L):\n",
    "        \"\"\"\n",
    "        returns: (..., c, L) where c is number of channels (default 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Materialize parameters\n",
    "        dt = torch.exp(self.log_dt)  # (H)\n",
    "        C = torch.view_as_complex(self.C)  # (H N)\n",
    "        A = -torch.exp(self.log_A_real) + 1j * self.A_imag  # (H N)\n",
    "\n",
    "        # Vandermonde multiplication\n",
    "        dtA = A * dt.unsqueeze(-1)  # (H N)\n",
    "        K = dtA.unsqueeze(-1) * torch.arange(L, device=A.device)  # (H N L)\n",
    "        C = C * (torch.exp(dtA) - 1.) / A\n",
    "        K = 2 * torch.einsum('hn, hnl -> hl', C, torch.exp(K)).real\n",
    "\n",
    "        return K\n",
    "\n",
    "    def register(self, name, tensor, lr=None):\n",
    "        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n",
    "\n",
    "        if lr == 0.0:\n",
    "            self.register_buffer(name, tensor)\n",
    "        else:\n",
    "            self.register_parameter(name, nn.Parameter(tensor))\n",
    "\n",
    "            optim = {\"weight_decay\": 0.0}\n",
    "            if lr is not None: optim[\"lr\"] = lr\n",
    "            setattr(getattr(self, name), \"_optim\", optim)\n",
    "\n",
    "\n",
    "class S4D(nn.Module):\n",
    "    def __init__(self, hidden_dim, d_state=64, dropout=0.0, transposed=True, **kernel_args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = hidden_dim\n",
    "        self.n = d_state\n",
    "        self.output_dim = self.h\n",
    "        self.transposed = transposed\n",
    "\n",
    "        self.D = nn.Parameter(torch.randn(self.h))\n",
    "\n",
    "        # SSM Kernel\n",
    "        self.kernel = S4DKernel(self.h, N=self.n, **kernel_args)\n",
    "\n",
    "        # Pointwise\n",
    "        self.activation = nn.GELU()\n",
    "        # dropout_fn = nn.Dropout2d # NOTE: bugged in PyTorch 1.11\n",
    "        dropout_fn = DropoutNd\n",
    "        self.dropout = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()\n",
    "\n",
    "        # position-wise output transform to mix features\n",
    "        self.output_linear = nn.Sequential(\n",
    "            nn.Conv1d(self.h, 2 * self.h, kernel_size=1),\n",
    "            nn.GLU(dim=-2),\n",
    "        )\n",
    "\n",
    "    def forward(self, u, **kwargs):  # absorbs return_output and transformer src mask\n",
    "        \"\"\" Input and output shape (B, H, L) \"\"\"\n",
    "        if not self.transposed: u = u.transpose(-1, -2)\n",
    "        L = u.size(-1)\n",
    "\n",
    "        # Compute SSM Kernel\n",
    "        k = self.kernel(L=L)  # (H L)\n",
    "\n",
    "        # Convolution\n",
    "        k_f = torch.fft.rfft(k, n=2 * L)  # (H L)\n",
    "        u_f = torch.fft.rfft(u, n=2 * L)  # (B H L)\n",
    "        y = torch.fft.irfft(u_f * k_f, n=2 * L)[..., :L]  # (B H L)\n",
    "\n",
    "        # Compute D term in state space equation - essentially a skip connection\n",
    "        y = y + u * self.D.unsqueeze(-1)\n",
    "\n",
    "        y = self.dropout(self.activation(y))\n",
    "        y = self.output_linear(y)\n",
    "        if not self.transposed: y = y.transpose(-1, -2)\n",
    "        return y, None\n",
    "\n",
    "\n",
    "class S4Model(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim,\n",
    "            vocab_size,\n",
    "            output_dim,\n",
    "            hidden_dim=256,\n",
    "            num_layers=4,\n",
    "            dropout=0.1,\n",
    "            lr=0.001,\n",
    "            prenorm=False,\n",
    "            finetune=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.prenorm = prenorm\n",
    "\n",
    "        # Linear encoder (embed_dim = 1 for grayscale and 3 for RGB)\n",
    "        self.encoder = nn.Linear(embed_dim, hidden_dim)\n",
    "\n",
    "        # Stack S4 layers as residual blocks\n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.s4_layers.append(\n",
    "                S4D(hidden_dim, dropout=dropout, transposed=True, lr=lr)\n",
    "            )\n",
    "            self.norms.append(nn.LayerNorm(hidden_dim))\n",
    "            self.dropouts.append(nn.Dropout(dropout))\n",
    "\n",
    "        # Linear decoder\n",
    "        self.decoder = nn.Linear(hidden_dim, output_dim)\n",
    "        self.finetune = finetune\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, embed_dim)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)  # -> (B, L, embed_dim)\n",
    "\n",
    "        x = self.encoder(x)  # (B, L, embed_dim) -> (B, L, hidden_dim)\n",
    "\n",
    "        x = x.transpose(-1, -2)  # (B, L, hidden_dim) -> (B, hidden_dim, L)\n",
    "        for layer, norm, dropout in zip(self.s4_layers, self.norms, self.dropouts):\n",
    "            # Each iteration of this loop will map (B, hidden_dim, L) -> (B, hidden_dim, L)\n",
    "\n",
    "            z = x\n",
    "            if self.prenorm:\n",
    "                # Prenorm\n",
    "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "            # Apply S4 block: we ignore the state input and output\n",
    "            z, _ = layer(z)\n",
    "\n",
    "            # Dropout on the output of the S4 block\n",
    "            z = dropout(z)\n",
    "\n",
    "            # Residual connection\n",
    "            x = z + x\n",
    "\n",
    "            if not self.prenorm:\n",
    "                # Postnorm\n",
    "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # Pooling: average pooling over the sequence length\n",
    "        if self.finetune:\n",
    "            x = x.mean(dim=1)\n",
    "            # Decode the outputs\n",
    "            x = self.decoder(x)  # (B, hidden_dim) -> (B, output_dim)\n",
    "            return x.squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            x = self.decoder(x)  # (B, hidden_dim) -> (B, output_dim)\n",
    "            return x.squeeze(-1).permute(1, 0, 2)\n",
    "\n",
    "    @cached_property\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ8vUdpWlDFH"
   },
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "geTAyd9vlFE_",
    "ExecuteTime": {
     "end_time": "2024-08-03T13:39:26.420799Z",
     "start_time": "2024-08-03T13:39:26.397698Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = 1 / math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "\n",
    "        qkv = self.qkv_proj(x)  # (batch_size, seq_length, embed_dim * 3)\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_length, 3 * head_dim)\n",
    "\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # each will be (batch_size, num_heads, seq_length, head_dim)\n",
    "\n",
    "        attn_scores = torch.einsum('bnqd,bnkd->bnqk', q,\n",
    "                                   k) * self.scale  # (batch_size, num_heads, seq_length, seq_length)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_probs = nn.functional.softmax(attn_scores, dim=-1)  # (batch_size, num_heads, seq_length, seq_length)\n",
    "\n",
    "        attn_output = torch.einsum('bnqk,bnvd->bnqd', attn_probs, v)  # (batch_size, num_heads, seq_length, head_dim)\n",
    "        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim)  # (batch_size, seq_length, embed_dim)\n",
    "\n",
    "        output = self.o_proj(attn_output)  # (batch_size, seq_length, embed_dim)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = self.layernorm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, hidden_dim, output_dim, dropout=0.1,\n",
    "                 finetune=True):\n",
    "        super(CustomTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, hidden_dim, dropout)\n",
    "            for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(embed_dim, output_dim)\n",
    "        self.finetune = finetune\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).T\n",
    "        mask = mask.int()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        mask = self.generate_square_subsequent_mask(seq_len).to(x.device)\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, mask)\n",
    "        if self.finetune:\n",
    "            x = x.mean(dim=1)\n",
    "            x = self.fc_out(x)\n",
    "            return x.squeeze(1)\n",
    "        else:\n",
    "            x = self.fc_out(x)\n",
    "            return x.squeeze(2)\n",
    "\n",
    "    @cached_property\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paOmrocTl2Ju"
   },
   "source": [
    "Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "7dvePJXxl3Va",
    "ExecuteTime": {
     "end_time": "2024-08-03T13:39:26.423565Z",
     "start_time": "2024-08-03T13:39:26.410234Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup basic configuration for logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)  # Ensure logs are directed to stdout\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def setup_logger(name=__name__):\n",
    "    logger = logging.getLogger(name)\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kEeq75VlJ8m"
   },
   "source": [
    "Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFmWPi0lMqG"
   },
   "source": [
    "IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "rTyW58kvlLAA",
    "ExecuteTime": {
     "end_time": "2024-08-03T13:39:26.426141Z",
     "start_time": "2024-08-03T13:39:26.421749Z"
    }
   },
   "outputs": [],
   "source": [
    "class IMDBDataset:\n",
    "    def __init__(self, tokenizer=None, tokenizer_name=\"bert-base-uncased\"):\n",
    "        if tokenizer is None:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(\n",
    "                tokenizer_name\n",
    "            )\n",
    "        else:\n",
    "            self.tokenizer = tokenizer\n",
    "\n",
    "    def text_pipeline(self, text):\n",
    "        # Encode the text using the BERT tokenizer\n",
    "        return self.tokenizer.encode(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",  # Optional: You can also handle padding in `collate_batch`\n",
    "            max_length=512,  # Typical max length for BERT\n",
    "            return_tensors=\"pt\",\n",
    "        ).squeeze(\n",
    "            0\n",
    "        )  # Remove batch dimension\n",
    "\n",
    "    def label_pipeline(self, label):\n",
    "        # In this dataset the labels are 1 for negative and 2 for positive\n",
    "        return label - 1\n",
    "\n",
    "    def collate_batch(self, batch):\n",
    "        label_list, text_list = [], []\n",
    "        for _label, _text in batch:\n",
    "            label_list.append(self.label_pipeline(_label))\n",
    "            processed_text = self.text_pipeline(_text)\n",
    "            text_list.append(processed_text)\n",
    "        return torch.tensor(label_list, dtype=torch.int64), pad_sequence(\n",
    "            text_list, padding_value=self.tokenizer.pad_token_id, batch_first=True\n",
    "        )\n",
    "\n",
    "    def get_dataloaders(self, batch_size):\n",
    "        train_iter, test_iter = IMDB(split=\"train\"), IMDB(\n",
    "            split=\"test\"\n",
    "        )\n",
    "        train_dataloader = DataLoader(\n",
    "            list(train_iter),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.collate_batch,\n",
    "        )\n",
    "        test_dataloader = DataLoader(\n",
    "            list(test_iter),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.collate_batch,\n",
    "        )\n",
    "        return train_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qPZsIz1lO09"
   },
   "source": [
    "Wikitext Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "VXA027TSlQ-g",
    "ExecuteTime": {
     "end_time": "2024-08-03T13:39:26.453840Z",
     "start_time": "2024-08-03T13:39:26.429783Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = setup_logger(__name__)\n",
    "\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, split, tokenizer=None, tokenizer_name=\"bert-base-uncased\"):\n",
    "        self.dataset = load_dataset(\n",
    "            \"Salesforce/wikitext\", \"wikitext-103-raw-v1\", split=split\n",
    "        ).filter(lambda x: x[\"text\"].strip() != \"\")\n",
    "\n",
    "        if tokenizer is None:\n",
    "            # Load pre-trained BERT tokenizer\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "            logger.info(f\"Pre-trained BERT tokenizer loaded: {tokenizer_name}\")\n",
    "        else:\n",
    "            self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx][\"text\"]\n",
    "\n",
    "    def text_pipeline(self, text):\n",
    "        # Encode text and return token ids\n",
    "        return self.tokenizer.encode(\n",
    "            text, truncation=True, padding=False, return_tensors=\"pt\"\n",
    "        ).squeeze(\n",
    "            0\n",
    "        )  # Remove the batch dimension\n",
    "\n",
    "    def collate_batch(self, batch):\n",
    "        text_list = [self.text_pipeline(text) for text in batch]\n",
    "        # Padding is handled by pad_sequence\n",
    "        padded_text_list = pad_sequence(\n",
    "            text_list, padding_value=self.tokenizer.pad_token_id, batch_first=True\n",
    "        )\n",
    "        # Dummy labels for wikitext, as it's often used for language modeling\n",
    "        return torch.zeros(len(text_list), dtype=torch.int64), padded_text_list\n",
    "\n",
    "    def get_dataloaders(self, batch_size):\n",
    "        train_dataset = WikiTextDataset(split=\"train\", tokenizer=self.tokenizer)\n",
    "        test_dataset = WikiTextDataset(split=\"test\", tokenizer=self.tokenizer)\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.collate_batch,\n",
    "        )\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.collate_batch,\n",
    "        )\n",
    "        return train_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV_iO_pzlYhW"
   },
   "source": [
    "Train Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjDcD1ODlcOk"
   },
   "source": [
    "Pretrain Train_Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "iWiuyv5Jlarg",
    "ExecuteTime": {
     "end_time": "2024-08-03T13:39:26.459161Z",
     "start_time": "2024-08-03T13:39:26.436797Z"
    }
   },
   "outputs": [],
   "source": [
    "def pretrain_train(model, dataloader, criterion, optimizer, device, epoch, logger):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (_, texts) in enumerate(dataloader):\n",
    "        texts = texts.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_texts = texts[:, :-1]\n",
    "        target_texts = texts[:, 1:]\n",
    "\n",
    "        output = model(input_texts)\n",
    "        loss = criterion(output.reshape(-1, output.size(-1)), target_texts.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 250 == 0:\n",
    "            logger.info(\n",
    "                f\"Train Epoch: {epoch} [{batch_idx * len(texts)}/{len(dataloader.dataset)} \"\n",
    "                f\"({100. * batch_idx / len(dataloader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
    "            )\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    logger.info(f\"====> Epoch: {epoch} Average loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def pretrain_evaluate(model, dataloader, criterion, device, logger):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (_, texts) in enumerate(dataloader):\n",
    "            texts = texts.to(device)\n",
    "\n",
    "            # Shift the target texts by one time step for the decoder input\n",
    "            input_texts = texts[:, :-1]\n",
    "            target_texts = texts[:, 1:]\n",
    "\n",
    "            output = model(input_texts)\n",
    "            loss = criterion(\n",
    "                output.reshape(-1, output.size(-1)), target_texts.reshape(-1)\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 250 == 0:\n",
    "                logger.info(\n",
    "                    f\"Eval Batch: {batch_idx + 1}/{len(dataloader)}\\tLoss: {loss.item():.6f}\"\n",
    "                )\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    logger.info(f\"====> Test set loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ0ksxkwlf8j"
   },
   "source": [
    "Train Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "UEc-mxPolhQZ",
    "ExecuteTime": {
     "end_time": "2024-08-03T13:39:26.459507Z",
     "start_time": "2024-08-03T13:39:26.442587Z"
    }
   },
   "outputs": [],
   "source": [
    "def finetune_train(model, dataloader, criterion, optimizer, device, epoch, logger):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (labels, texts) in enumerate(dataloader):\n",
    "        labels, texts = labels.to(device), texts.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(texts)\n",
    "        loss = criterion(output, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 250 == 0:\n",
    "            logger.info(\n",
    "                f\"Train Epoch: {epoch} [{batch_idx * len(labels)}/{len(dataloader.dataset)} \"\n",
    "                f\"({100. * batch_idx / len(dataloader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    logger.info(f\"====> Epoch: {epoch} Average loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def finetune_evaluate(model, dataloader, criterion, device, logger):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (labels, texts) in enumerate(dataloader):\n",
    "            labels, texts = labels.to(device), texts.to(device)\n",
    "            output = model(texts)\n",
    "            loss = criterion(output, labels.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (output > 0).float()\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            if batch_idx % 250 == 0:\n",
    "                logger.info(\n",
    "                    f\"Eval Batch: {batch_idx + 1}/{len(dataloader)}\\tLoss: {loss.item():.6f}\"\n",
    "                )\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    logger.info(f\"====> Test set loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVZWzbollybG"
   },
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbGe5G3hlGCI"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "tuYV0cXGlz7_",
    "ExecuteTime": {
     "end_time": "2024-08-03T13:39:26.467648Z",
     "start_time": "2024-08-03T13:39:26.459379Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"models\": {\n",
    "    \"lstm\": {\n",
    "      \"embed_dim\": 32,\n",
    "      \"hidden_dim\": 96,\n",
    "      \"num_layers\": 1,\n",
    "      \"output_dim\": 1\n",
    "    },\n",
    "    \"s4\": {\n",
    "      \"embed_dim\": 24,\n",
    "      \"hidden_dim\": 128,\n",
    "      \"output_dim\": 1,\n",
    "      \"num_layers\": 2\n",
    "    },\n",
    "    \"transformer\": {\n",
    "      \"embed_dim\": 32,\n",
    "      \"num_heads\": 2,\n",
    "      \"hidden_dim\": 128,\n",
    "      \"output_dim\": 1,\n",
    "      \"num_layers\": 4\n",
    "    }\n",
    "  },\n",
    "  \"run_parameters\": {\n",
    "    \"batch_size\": 2,\n",
    "    \"n_epochs\": 50,\n",
    "    \"learning_rate\": 0.001\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22InhBYbl6Ye"
   },
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "9DhoXleNl7Vh",
    "ExecuteTime": {
     "end_time": "2024-08-03T13:39:26.489234Z",
     "start_time": "2024-08-03T13:39:26.464337Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_final_layer(model, config, model_name, device):\n",
    "    hidden_dim = config[\"models\"][model_name][\"hidden_dim\"]\n",
    "    embed_dim = config[\"models\"][model_name][\"embed_dim\"]\n",
    "    if model_name == \"lstm\":\n",
    "        output_dim = 1\n",
    "        model.Wy = torch.nn.Parameter(torch.empty(output_dim, hidden_dim).to(device))\n",
    "        model.by = torch.nn.Parameter(torch.zeros(output_dim, 1).to(device))\n",
    "        torch.nn.init.xavier_uniform_(model.Wy)\n",
    "    elif model_name == \"s4\":\n",
    "        output_dim = 1\n",
    "        model.decoder = torch.nn.Linear(hidden_dim, output_dim).to(device)\n",
    "    else:\n",
    "        output_dim = 1\n",
    "        model.fc_out = torch.nn.Linear(embed_dim, output_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayIrsAyEl89H"
   },
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sey0XXal912",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "outputId": "7da61667-815f-4492-b859-341f4d2ca826",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-08-03T13:39:26.476401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-03 16:39:27 - __main__ - INFO - parameter_cnt: 1026337\n",
      "2024-08-03 16:39:27 - __main__ - INFO - Starting training...\n",
      "2024-08-03 16:39:27 - __main__ - INFO - Train Epoch: 0 [0/25000 (0%)]\tLoss: 0.633421\n"
     ]
    }
   ],
   "source": [
    "possible_models = list(config[\"models\"].keys())\n",
    "run_types = [\n",
    "        \"task\",\n",
    "        \"lra_pretrain\",\n",
    "        \"wikitext_pretrain\",\n",
    "        \"task_finetune_lra_pretrain\",\n",
    "        \"task_finetune_wikitext_pretrain\",\n",
    "    ]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    model: str\n",
    "    run_type: str\n",
    "    use_wandb: bool\n",
    "\n",
    "args = Args(model=\"lstm\", run_type=\"task\", use_wandb=False)\n",
    "\n",
    "model_config = config[\"models\"][args.model]\n",
    "run_parameters = config[\"run_parameters\"]\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "run_name = f\"run_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{args.model}_{args.run_type}\"\n",
    "\n",
    "batch_size, n_epochs, learning_rate = (\n",
    "    run_parameters[\"batch_size\"],\n",
    "    run_parameters[\"n_epochs\"],\n",
    "    run_parameters[\"learning_rate\"],\n",
    ")\n",
    "\n",
    "# Load tokenizer and data loaders\n",
    "if \"wikitext\" in args.run_type and \"finetune\" in args.run_type:\n",
    "    wikitext_dataset = WikiTextDataset(split=\"train\")\n",
    "    tokenizer = wikitext_dataset.tokenizer\n",
    "    imdb_dataset = IMDBDataset(tokenizer=tokenizer)\n",
    "    train_dataloader, test_dataloader = imdb_dataset.get_dataloaders(batch_size)\n",
    "elif \"wikitext\" in args.run_type:\n",
    "    wikitext_dataset = WikiTextDataset(split=\"train\")\n",
    "    tokenizer = wikitext_dataset.tokenizer\n",
    "    train_dataloader, test_dataloader = wikitext_dataset.get_dataloaders(batch_size)\n",
    "else:\n",
    "    imdb_dataset = IMDBDataset()\n",
    "    tokenizer = imdb_dataset.tokenizer\n",
    "    train_dataloader, test_dataloader = imdb_dataset.get_dataloaders(batch_size)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "finetune = args.run_type in [\n",
    "    \"task\",\n",
    "    \"task_finetune_lra_pretrain\",\n",
    "    \"task_finetune_wikitext_pretrain\",\n",
    "]\n",
    "\n",
    "model_dic = {\n",
    "    \"lstm\": CustomLSTMModel,\n",
    "    \"s4\": S4Model,\n",
    "    \"transformer\": CustomTransformerModel,\n",
    "}\n",
    "\n",
    "if args.run_type == \"task\":\n",
    "    config[\"models\"][args.model][\"output_dim\"] = 1\n",
    "else:\n",
    "    config[\"models\"][args.model][\"output_dim\"] = vocab_size\n",
    "\n",
    "model = model_dic[args.model](\n",
    "    vocab_size=vocab_size, **config[\"models\"][args.model], finetune=finetune\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "pretrained = \"lra_pretrained\" if \"lra\" in args.run_type else \"wikitext_pretrained\"\n",
    "checkpoint_path = f\"{args.model}_{pretrained}.pth\"\n",
    "if finetune and args.run_type != \"task\":\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    logger.info(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "    replace_final_layer(model, config, args.model, device)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss() if finetune else torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "logger.info(f\"parameter_cnt: {model.count_parameters}\")\n",
    "\n",
    "if args.use_wandb:\n",
    "    # Initialize WandB\n",
    "    wandb.login(key=\"5fda0926085bc8963be5e43c4e501d992e35abe8\")\n",
    "    wandb.init(project=\"model-comparison\", name=run_name)\n",
    "\n",
    "    # Log hyperparameters and model\n",
    "    wandb.config.update(\n",
    "        {\n",
    "            **{\n",
    "                \"run_name\": run_name,\n",
    "                \"model\": args.model,\n",
    "                \"parameter_cnt\": model.count_parameters,\n",
    "            },\n",
    "            **config[\"models\"][args.model],\n",
    "            **config[\"run_parameters\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Training loop\n",
    "logger.info(\"Starting training...\")\n",
    "\n",
    "if finetune:\n",
    "    train, evaluate = finetune_train, finetune_evaluate\n",
    "else:\n",
    "    train, evaluate = pretrain_train,pretrain_evaluate\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "epochs_since_improvement = 0\n",
    "\n",
    "# add seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(\n",
    "        model, train_dataloader, criterion, optimizer, device, epoch, logger\n",
    "    )\n",
    "    test_evaluate_res = evaluate(model, test_dataloader, criterion, device, logger)\n",
    "\n",
    "    test_accuracy = None\n",
    "    if not finetune:\n",
    "        test_loss = test_evaluate_res\n",
    "    else:\n",
    "        test_loss, test_accuracy = test_evaluate_res\n",
    "    logger.info(\n",
    "        f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy}\"\n",
    "    )\n",
    "\n",
    "    if args.use_wandb:\n",
    "        # Log metrics to WandB\n",
    "        log_dict = {\"train_loss\": train_loss, \"test_loss\": test_loss}\n",
    "        if test_accuracy is not None:\n",
    "            log_dict[\"test_accuracy\"] = test_accuracy\n",
    "        wandb.log(log_dict)\n",
    "\n",
    "    # Check for improvement\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        epochs_since_improvement = 0\n",
    "        pretrained = (\n",
    "            \"lra_pretrained\" if \"lra\" in args.run_type else \"wikitext_pretrained\"\n",
    "        )\n",
    "        checkpoint_path = f\"{args.model}_{pretrained}.pth\"\n",
    "        if not finetune:\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            logger.info(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "        logger.info(\n",
    "            f\"New best model found at epoch {epoch} with test loss {test_loss:.4f}\"\n",
    "        )\n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "        logger.info(f\"No improvement for {epochs_since_improvement} epochs\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if epochs_since_improvement >= 5:\n",
    "        logger.info(f\"No improvement for 5 consecutive epochs. Stopping training.\")\n",
    "        break\n",
    "\n",
    "logger.info(\"Training completed.\")\n",
    "\n",
    "if args.use_wandb:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
