{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8a71iASq9Gja",
        "outputId": "7e487009-c596-49ce-a7e2-cdc88cd0eade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.4.0%2Bcu118-cp310-cp310-linux_x86_64.whl (857.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.7/857.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.20.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.20.5-py3-none-manylinux2014_x86_64.whl (142.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.9/142.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.0.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.3.1%2Bcu118-cp310-cp310-linux_x86_64.whl (839.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m839.7/839.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.7.0.84 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.20.5 nvidia-nvtx-cu11-11.8.86 torch-2.3.1+cu118\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZmMhUpmj_Jpd",
        "outputId": "ad017c96-a4ad-4197-87af-5fb3c5a6422d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.17.5-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting torchdata==0.7.1\n",
            "  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting portalocker==2.10.0\n",
            "  Downloading portalocker-2.10.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1) (2.31.0)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1) (2.3.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests (from torchdata==0.7.1)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.11.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.7.1) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.7.1) (2024.7.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata==0.7.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata==0.7.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata==0.7.1) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata==0.7.1) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata==0.7.1) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata==0.7.1) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata==0.7.1) (8.7.0.84)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata==0.7.1) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata==0.7.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata==0.7.1) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata==0.7.1) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata==0.7.1) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata==0.7.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata==0.7.1) (11.8.86)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata==0.7.1) (2.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata==0.7.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata==0.7.1) (1.3.0)\n",
            "Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.0-py3-none-any.whl (18 kB)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.17.5-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.11.0-py2.py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.6/303.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: xxhash, smmap, setproctitle, sentry-sdk, requests, pyarrow, portalocker, fsspec, einops, docker-pycreds, dill, multiprocess, gitdb, torchdata, gitpython, wandb, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 docker-pycreds-0.4.0 einops-0.8.0 fsspec-2024.5.0 gitdb-4.0.11 gitpython-3.1.43 multiprocess-0.70.16 portalocker-2.10.0 pyarrow-17.0.0 requests-2.32.3 sentry-sdk-2.11.0 setproctitle-1.3.3 smmap-5.0.1 torchdata-0.7.1 wandb-0.17.5 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install datasets wandb einops torchdata==0.7.1 portalocker==2.10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import datetime\n",
        "import random\n",
        "import logging\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import wandb\n",
        "from functools import cached_property\n",
        "from einops import rearrange, repeat\n",
        "from datasets import load_dataset\n",
        "import torchtext\n",
        "from torchtext.datasets import IMDB\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "torchtext.disable_torchtext_deprecation_warning()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJYGyX9Yk4eo"
      },
      "source": [
        "Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCQ3ZAGvk-MD"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AlllYobk6nj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class CustomLSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, output_dim, finetune=True):\n",
        "        super(CustomLSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_dim = embed_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = vocab_size\n",
        "        self.finetune = finetune\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Create layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            input_dim = embed_dim if i == 0 else hidden_dim\n",
        "            self.layers.append(LSTMLayer(input_dim, hidden_dim))\n",
        "\n",
        "        # Output layer weights and biases\n",
        "        self.Wy = nn.Parameter(torch.empty(output_dim, hidden_dim))\n",
        "        self.by = nn.Parameter(torch.zeros(output_dim, 1))\n",
        "\n",
        "        # Initialize weights using Xavier initialization\n",
        "        nn.init.xavier_uniform_(self.Wy)\n",
        "\n",
        "    def forward(self, texts):\n",
        "        batch_size, seq_len = texts.size()\n",
        "        embedded = self.embedding(texts).permute(1, 2, 0)  # (seq_len, embed_dim, batch_size)\n",
        "\n",
        "        h = [\n",
        "            torch.zeros(self.hidden_dim, batch_size).to(texts.device)\n",
        "            for _ in range(self.num_layers)\n",
        "        ]\n",
        "        c = [\n",
        "            torch.zeros(self.hidden_dim, batch_size).to(texts.device)\n",
        "            for _ in range(self.num_layers)\n",
        "        ]\n",
        "\n",
        "        if not self.finetune:\n",
        "            y = torch.zeros(seq_len, batch_size, self.vocab_size).to(texts.device)  # (seq_len, batch_size, vocab_size)\n",
        "            for t in range(seq_len):\n",
        "                x = embedded[t, :, :]  # (embed_dim, batch_size)\n",
        "                for i, layer in enumerate(self.layers):\n",
        "                    h[i], c[i] = layer(x, h[i], c[i])\n",
        "                    x = h[i]\n",
        "                h_last = h[-1].permute(1, 0)  # (batch_size, hidden_dim)\n",
        "                y[t] = torch.matmul(h_last, self.Wy.t()) + self.by.t()  # (batch_size, vocab_size)\n",
        "\n",
        "            return y.permute(1, 0, 2)\n",
        "        else:\n",
        "            for t in range(seq_len):\n",
        "                x = embedded[t, :, :]  # (embed_dim, batch_size)\n",
        "                for i, layer in enumerate(self.layers):\n",
        "                    h[i], c[i] = layer(x, h[i], c[i])\n",
        "                    x = h[i]\n",
        "\n",
        "            h_last = h[-1].permute(1, 0)  # (batch_size, hidden_dim)\n",
        "            y = torch.matmul(h_last, self.Wy.t()) + self.by.t()  # (batch_size, output_dim)\n",
        "            return y.squeeze(1)\n",
        "\n",
        "    @cached_property\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "class LSTMLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(LSTMLayer, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # LSTM weights and biases\n",
        "        self.Wf = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
        "        self.bf = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
        "        self.Wi = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
        "        self.bi = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
        "        self.Wo = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
        "        self.bo = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
        "        self.Wc = nn.Parameter(torch.empty(hidden_dim, input_dim + hidden_dim))\n",
        "        self.bc = nn.Parameter(torch.zeros(hidden_dim, 1))\n",
        "\n",
        "        # Initialize weights using Xavier initialization\n",
        "        nn.init.xavier_uniform_(self.Wf)\n",
        "        nn.init.xavier_uniform_(self.Wi)\n",
        "        nn.init.xavier_uniform_(self.Wo)\n",
        "        nn.init.xavier_uniform_(self.Wc)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return torch.tanh(x)\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        concat = torch.cat((h, x), dim=0)  # (hidden_dim + input_dim, batch_size)\n",
        "\n",
        "        ft = self.sigmoid(\n",
        "            torch.matmul(self.Wf, concat) + self.bf\n",
        "        )  # (hidden, batch_size)\n",
        "        it = self.sigmoid(\n",
        "            torch.matmul(self.Wi, concat) + self.bi\n",
        "        )  # (hidden, batch_size)\n",
        "        c_hat = self.tanh(\n",
        "            torch.matmul(self.Wc, concat) + self.bc\n",
        "        )  # (hidden, batch_size)\n",
        "        c = ft * c + it * c_hat  # (hidden, batch_size)\n",
        "        ot = self.sigmoid(\n",
        "            torch.matmul(self.Wo, concat) + self.bo\n",
        "        )  # (hidden, batch_size)\n",
        "        h = ot * self.tanh(c)\n",
        "\n",
        "        return h, c\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwSlNHbilAPX"
      },
      "source": [
        "S4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaiAg-gmlBVS"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class DropoutNd(nn.Module):\n",
        "    def __init__(self, p: float = 0.5, tie=True, transposed=True):\n",
        "        \"\"\"\n",
        "        tie: tie dropout mask across sequence lengths (Dropout1d/2d/3d)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if p < 0 or p >= 1:\n",
        "            raise ValueError(\"dropout probability has to be in [0, 1), \" \"but got {}\".format(p))\n",
        "        self.p = p\n",
        "        self.tie = tie\n",
        "        self.transposed = transposed\n",
        "        self.binomial = torch.distributions.binomial.Binomial(probs=1 - self.p)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"X: (batch, dim, lengths...).\"\"\"\n",
        "        if self.training:\n",
        "            if not self.transposed: X = rearrange(X, 'b ... d -> b d ...')\n",
        "            # binomial = torch.distributions.binomial.Binomial(probs=1-self.p) # This is incredibly slow because of CPU -> GPU copying\n",
        "            mask_shape = X.shape[:2] + (1,) * (X.ndim - 2) if self.tie else X.shape\n",
        "            # mask = self.binomial.sample(mask_shape)\n",
        "            mask = torch.rand(*mask_shape, device=X.device) < 1. - self.p\n",
        "            X = X * mask * (1.0 / (1 - self.p))\n",
        "            if not self.transposed: X = rearrange(X, 'b d ... -> b ... d')\n",
        "            return X\n",
        "        return X\n",
        "\n",
        "\n",
        "class S4DKernel(nn.Module):\n",
        "    \"\"\"Generate convolution kernel from diagonal SSM parameters.\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim, N=64, dt_min=0.001, dt_max=0.1, lr=None):\n",
        "        super().__init__()\n",
        "        # Generate dt\n",
        "        H = hidden_dim\n",
        "        log_dt = torch.rand(H) * (\n",
        "                math.log(dt_max) - math.log(dt_min)\n",
        "        ) + math.log(dt_min)\n",
        "\n",
        "        C = torch.randn(H, N // 2, dtype=torch.cfloat)\n",
        "        self.C = nn.Parameter(torch.view_as_real(C))\n",
        "        self.register(\"log_dt\", log_dt, lr)\n",
        "\n",
        "        log_A_real = torch.log(0.5 * torch.ones(H, N // 2))\n",
        "        A_imag = math.pi * repeat(torch.arange(N // 2), 'n -> h n', h=H)\n",
        "        self.register(\"log_A_real\", log_A_real, lr)\n",
        "        self.register(\"A_imag\", A_imag, lr)\n",
        "\n",
        "    def forward(self, L):\n",
        "        \"\"\"\n",
        "        returns: (..., c, L) where c is number of channels (default 1)\n",
        "        \"\"\"\n",
        "\n",
        "        # Materialize parameters\n",
        "        dt = torch.exp(self.log_dt)  # (H)\n",
        "        C = torch.view_as_complex(self.C)  # (H N)\n",
        "        A = -torch.exp(self.log_A_real) + 1j * self.A_imag  # (H N)\n",
        "\n",
        "        # Vandermonde multiplication\n",
        "        dtA = A * dt.unsqueeze(-1)  # (H N)\n",
        "        K = dtA.unsqueeze(-1) * torch.arange(L, device=A.device)  # (H N L)\n",
        "        C = C * (torch.exp(dtA) - 1.) / A\n",
        "        K = 2 * torch.einsum('hn, hnl -> hl', C, torch.exp(K)).real\n",
        "\n",
        "        return K\n",
        "\n",
        "    def register(self, name, tensor, lr=None):\n",
        "        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n",
        "\n",
        "        if lr == 0.0:\n",
        "            self.register_buffer(name, tensor)\n",
        "        else:\n",
        "            self.register_parameter(name, nn.Parameter(tensor))\n",
        "\n",
        "            optim = {\"weight_decay\": 0.0}\n",
        "            if lr is not None: optim[\"lr\"] = lr\n",
        "            setattr(getattr(self, name), \"_optim\", optim)\n",
        "\n",
        "\n",
        "class S4D(nn.Module):\n",
        "    def __init__(self, hidden_dim, d_state=64, dropout=0.0, transposed=True, **kernel_args):\n",
        "        super().__init__()\n",
        "\n",
        "        self.h = hidden_dim\n",
        "        self.n = d_state\n",
        "        self.output_dim = self.h\n",
        "        self.transposed = transposed\n",
        "\n",
        "        self.D = nn.Parameter(torch.randn(self.h))\n",
        "\n",
        "        # SSM Kernel\n",
        "        self.kernel = S4DKernel(self.h, N=self.n, **kernel_args)\n",
        "\n",
        "        # Pointwise\n",
        "        self.activation = nn.GELU()\n",
        "        # dropout_fn = nn.Dropout2d # NOTE: bugged in PyTorch 1.11\n",
        "        dropout_fn = DropoutNd\n",
        "        self.dropout = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()\n",
        "\n",
        "        # position-wise output transform to mix features\n",
        "        self.output_linear = nn.Sequential(\n",
        "            nn.Conv1d(self.h, 2 * self.h, kernel_size=1),\n",
        "            nn.GLU(dim=-2),\n",
        "        )\n",
        "\n",
        "    def forward(self, u, **kwargs):  # absorbs return_output and transformer src mask\n",
        "        \"\"\" Input and output shape (B, H, L) \"\"\"\n",
        "        if not self.transposed: u = u.transpose(-1, -2)\n",
        "        L = u.size(-1)\n",
        "\n",
        "        # Compute SSM Kernel\n",
        "        k = self.kernel(L=L)  # (H L)\n",
        "\n",
        "        # Convolution\n",
        "        k_f = torch.fft.rfft(k, n=2 * L)  # (H L)\n",
        "        u_f = torch.fft.rfft(u, n=2 * L)  # (B H L)\n",
        "        y = torch.fft.irfft(u_f * k_f, n=2 * L)[..., :L]  # (B H L)\n",
        "\n",
        "        # Compute D term in state space equation - essentially a skip connection\n",
        "        y = y + u * self.D.unsqueeze(-1)\n",
        "\n",
        "        y = self.dropout(self.activation(y))\n",
        "        y = self.output_linear(y)\n",
        "        if not self.transposed: y = y.transpose(-1, -2)\n",
        "        return y, None\n",
        "\n",
        "\n",
        "class S4Model(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embed_dim,\n",
        "            vocab_size,\n",
        "            output_dim,\n",
        "            hidden_dim=256,\n",
        "            num_layers=4,\n",
        "            dropout=0.1,\n",
        "            lr=0.001,\n",
        "            prenorm=False,\n",
        "            finetune=True\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.prenorm = prenorm\n",
        "\n",
        "        # Linear encoder (embed_dim = 1 for grayscale and 3 for RGB)\n",
        "        self.encoder = nn.Linear(embed_dim, hidden_dim)\n",
        "\n",
        "        # Stack S4 layers as residual blocks\n",
        "        self.s4_layers = nn.ModuleList()\n",
        "        self.norms = nn.ModuleList()\n",
        "        self.dropouts = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.s4_layers.append(\n",
        "                S4D(hidden_dim, dropout=dropout, transposed=True, lr=lr)\n",
        "            )\n",
        "            self.norms.append(nn.LayerNorm(hidden_dim))\n",
        "            self.dropouts.append(nn.Dropout(dropout))\n",
        "\n",
        "        # Linear decoder\n",
        "        self.decoder = nn.Linear(hidden_dim, output_dim)\n",
        "        self.finetune = finetune\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Input x is shape (B, L, embed_dim)\n",
        "        \"\"\"\n",
        "        x = self.embedding(x)  # -> (B, L, embed_dim)\n",
        "\n",
        "        x = self.encoder(x)  # (B, L, embed_dim) -> (B, L, hidden_dim)\n",
        "\n",
        "        x = x.transpose(-1, -2)  # (B, L, hidden_dim) -> (B, hidden_dim, L)\n",
        "        for layer, norm, dropout in zip(self.s4_layers, self.norms, self.dropouts):\n",
        "            # Each iteration of this loop will map (B, hidden_dim, L) -> (B, hidden_dim, L)\n",
        "\n",
        "            z = x\n",
        "            if self.prenorm:\n",
        "                # Prenorm\n",
        "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
        "\n",
        "            # Apply S4 block: we ignore the state input and output\n",
        "            z, _ = layer(z)\n",
        "\n",
        "            # Dropout on the output of the S4 block\n",
        "            z = dropout(z)\n",
        "\n",
        "            # Residual connection\n",
        "            x = z + x\n",
        "\n",
        "            if not self.prenorm:\n",
        "                # Postnorm\n",
        "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
        "\n",
        "        x = x.transpose(-1, -2)\n",
        "\n",
        "        # Pooling: average pooling over the sequence length\n",
        "        if self.finetune:\n",
        "            x = x.mean(dim=1)\n",
        "            # Decode the outputs\n",
        "            x = self.decoder(x)  # (B, hidden_dim) -> (B, output_dim)\n",
        "            return x.squeeze(-1)\n",
        "\n",
        "        else:\n",
        "            x = self.decoder(x)  # (B, hidden_dim) -> (B, output_dim)\n",
        "            return x.squeeze(-1).permute(1, 0, 2)\n",
        "\n",
        "    @cached_property\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ8vUdpWlDFH"
      },
      "source": [
        "Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geTAyd9vlFE_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.scale = 1 / math.sqrt(self.head_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_length, embed_dim = x.size()\n",
        "\n",
        "        qkv = self.qkv_proj(x)  # (batch_size, seq_length, embed_dim * 3)\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_length, 3 * head_dim)\n",
        "\n",
        "        q, k, v = qkv.chunk(3, dim=-1)  # each will be (batch_size, num_heads, seq_length, head_dim)\n",
        "\n",
        "        attn_scores = torch.einsum('bnqd,bnkd->bnqk', q,\n",
        "                                   k) * self.scale  # (batch_size, num_heads, seq_length, seq_length)\n",
        "\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn_probs = nn.functional.softmax(attn_scores, dim=-1)  # (batch_size, num_heads, seq_length, seq_length)\n",
        "\n",
        "        attn_output = torch.einsum('bnqk,bnvd->bnqd', attn_probs, v)  # (batch_size, num_heads, seq_length, head_dim)\n",
        "        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim)  # (batch_size, seq_length, embed_dim)\n",
        "\n",
        "        output = self.o_proj(attn_output)  # (batch_size, seq_length, embed_dim)\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.attention(x, mask)\n",
        "        x = self.layernorm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feedforward(x)\n",
        "        x = self.layernorm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "\n",
        "class CustomTransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, hidden_dim, output_dim, dropout=0.1,\n",
        "                 finetune=True):\n",
        "        super(CustomTransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embed_dim)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, hidden_dim, dropout)\n",
        "            for _ in range(num_layers)])\n",
        "        self.fc_out = nn.Linear(embed_dim, output_dim)\n",
        "        self.finetune = finetune\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).T\n",
        "        mask = mask.int()\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size()\n",
        "        mask = self.generate_square_subsequent_mask(seq_len).to(x.device)\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, mask)\n",
        "        if self.finetune:\n",
        "            x = x.mean(dim=1)\n",
        "            x = self.fc_out(x)\n",
        "            return x.squeeze(1)\n",
        "        else:\n",
        "            x = self.fc_out(x)\n",
        "            return x.squeeze(2)\n",
        "\n",
        "    @cached_property\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kEeq75VlJ8m"
      },
      "source": [
        "Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgFmWPi0lMqG"
      },
      "source": [
        "IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTyW58kvlLAA",
        "outputId": "f1189865-84e9-4cd0-9110-b5eb333d8e24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "class IMDBDataset:\n",
        "    script_dir = os.path.dirname(os.path.realpath(__file__))\n",
        "    cache_dir = os.path.join(script_dir, \"../saved\")\n",
        "\n",
        "    def __init__(self, tokenizer=None, tokenizer_name=\"bert-base-uncased\"):\n",
        "        if tokenizer is None:\n",
        "            self.tokenizer = BertTokenizer.from_pretrained(\n",
        "                tokenizer_name, cache_dir=IMDBDataset.cache_dir\n",
        "            )\n",
        "        else:\n",
        "            self.tokenizer = tokenizer\n",
        "\n",
        "    def text_pipeline(self, text):\n",
        "        # Encode the text using the BERT tokenizer\n",
        "        return self.tokenizer.encode(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",  # Optional: You can also handle padding in `collate_batch`\n",
        "            max_length=512,  # Typical max length for BERT\n",
        "            return_tensors=\"pt\",\n",
        "        ).squeeze(\n",
        "            0\n",
        "        )  # Remove batch dimension\n",
        "\n",
        "    def label_pipeline(self, label):\n",
        "        # In this dataset the labels are 1 for negative and 2 for positive\n",
        "        return label - 1\n",
        "\n",
        "    def collate_batch(self, batch):\n",
        "        label_list, text_list = [], []\n",
        "        for _label, _text in batch:\n",
        "            label_list.append(self.label_pipeline(_label))\n",
        "            processed_text = self.text_pipeline(_text)\n",
        "            text_list.append(processed_text)\n",
        "        return torch.tensor(label_list, dtype=torch.int64), pad_sequence(\n",
        "            text_list, padding_value=self.tokenizer.pad_token_id, batch_first=True\n",
        "        )\n",
        "\n",
        "    def get_dataloaders(self, batch_size):\n",
        "        train_iter, test_iter = IMDB(split=\"train\", root=IMDBDataset.cache_dir), IMDB(\n",
        "            split=\"test\", root=IMDBDataset.cache_dir\n",
        "        )\n",
        "        train_dataloader = DataLoader(\n",
        "            list(train_iter),\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=self.collate_batch,\n",
        "        )\n",
        "        test_dataloader = DataLoader(\n",
        "            list(test_iter),\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=self.collate_batch,\n",
        "        )\n",
        "        return train_dataloader, test_dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qPZsIz1lO09"
      },
      "source": [
        "Wikitext Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXA027TSlQ-g"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize logger\n",
        "logger = setup_logger(__name__)\n",
        "\n",
        "\n",
        "class WikiTextDataset(Dataset):\n",
        "    script_dir = os.path.dirname(os.path.realpath(__file__))\n",
        "    cache_dir = os.path.join(script_dir, \"../saved\")\n",
        "\n",
        "    def __init__(self, split, tokenizer=None, tokenizer_name=\"bert-base-uncased\"):\n",
        "        self.dataset = load_dataset(\n",
        "            \"Salesforce/wikitext\", \"wikitext-103-raw-v1\", split=split, cache_dir=WikiTextDataset.cache_dir\n",
        "        ).filter(lambda x: x[\"text\"].strip() != \"\")\n",
        "\n",
        "        if tokenizer is None:\n",
        "            # Load pre-trained BERT tokenizer\n",
        "            self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name,cache_dir=WikiTextDataset.cache_dir)\n",
        "            logger.info(f\"Pre-trained BERT tokenizer loaded: {tokenizer_name}\")\n",
        "        else:\n",
        "            self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset[idx][\"text\"]\n",
        "\n",
        "    def text_pipeline(self, text):\n",
        "        # Encode text and return token ids\n",
        "        return self.tokenizer.encode(\n",
        "            text, truncation=True, padding=False, return_tensors=\"pt\"\n",
        "        ).squeeze(\n",
        "            0\n",
        "        )  # Remove the batch dimension\n",
        "\n",
        "    def collate_batch(self, batch):\n",
        "        text_list = [self.text_pipeline(text) for text in batch]\n",
        "        # Padding is handled by pad_sequence\n",
        "        padded_text_list = pad_sequence(\n",
        "            text_list, padding_value=self.tokenizer.pad_token_id, batch_first=True\n",
        "        )\n",
        "        # Dummy labels for wikitext, as it's often used for language modeling\n",
        "        return torch.zeros(len(text_list), dtype=torch.int64), padded_text_list\n",
        "\n",
        "    def get_dataloaders(self, batch_size):\n",
        "        train_dataset = WikiTextDataset(split=\"train\", tokenizer=self.tokenizer)\n",
        "        test_dataset = WikiTextDataset(split=\"test\", tokenizer=self.tokenizer)\n",
        "\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=self.collate_batch,\n",
        "        )\n",
        "        test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=self.collate_batch,\n",
        "        )\n",
        "        return train_dataloader, test_dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV_iO_pzlYhW"
      },
      "source": [
        "Train Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjDcD1ODlcOk"
      },
      "source": [
        "Pretrain Train_Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWiuyv5Jlarg"
      },
      "outputs": [],
      "source": [
        "def pretrain_train(model, dataloader, criterion, optimizer, device, epoch, logger):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (_, texts) in enumerate(dataloader):\n",
        "        texts = texts.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_texts = texts[:, :-1]\n",
        "        target_texts = texts[:, 1:]\n",
        "\n",
        "        output = model(input_texts)\n",
        "        loss = criterion(output.reshape(-1, output.size(-1)), target_texts.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 250 == 0:\n",
        "            logger.info(\n",
        "                f\"Train Epoch: {epoch} [{batch_idx * len(texts)}/{len(dataloader.dataset)} \"\n",
        "                f\"({100. * batch_idx / len(dataloader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
        "            )\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    logger.info(f\"====> Epoch: {epoch} Average loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def pretrain_evaluate(model, dataloader, criterion, device, logger):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (_, texts) in enumerate(dataloader):\n",
        "            texts = texts.to(device)\n",
        "\n",
        "            # Shift the target texts by one time step for the decoder input\n",
        "            input_texts = texts[:, :-1]\n",
        "            target_texts = texts[:, 1:]\n",
        "\n",
        "            output = model(input_texts)\n",
        "            loss = criterion(\n",
        "                output.reshape(-1, output.size(-1)), target_texts.reshape(-1)\n",
        "            )\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 250 == 0:\n",
        "                logger.info(\n",
        "                    f\"Eval Batch: {batch_idx + 1}/{len(dataloader)}\\tLoss: {loss.item():.6f}\"\n",
        "                )\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    logger.info(f\"====> Test set loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ0ksxkwlf8j"
      },
      "source": [
        "Train Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEc-mxPolhQZ"
      },
      "outputs": [],
      "source": [
        "def finetune_train(model, dataloader, criterion, optimizer, device, epoch, logger):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (labels, texts) in enumerate(dataloader):\n",
        "        labels, texts = labels.to(device), texts.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(texts)\n",
        "        loss = criterion(output, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 250 == 0:\n",
        "            logger.info(\n",
        "                f\"Train Epoch: {epoch} [{batch_idx * len(labels)}/{len(dataloader.dataset)} \"\n",
        "                f\"({100. * batch_idx / len(dataloader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
        "            )\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    logger.info(f\"====> Epoch: {epoch} Average loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def finetune_evaluate(model, dataloader, criterion, device, logger):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (labels, texts) in enumerate(dataloader):\n",
        "            labels, texts = labels.to(device), texts.to(device)\n",
        "            output = model(texts)\n",
        "            loss = criterion(output, labels.float())\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = (output > 0).float()\n",
        "            correct_predictions += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            if batch_idx % 250 == 0:\n",
        "                logger.info(\n",
        "                    f\"Eval Batch: {batch_idx + 1}/{len(dataloader)}\\tLoss: {loss.item():.6f}\"\n",
        "                )\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    logger.info(f\"====> Test set loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVZWzbollybG"
      },
      "source": [
        "Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbGe5G3hlGCI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuYV0cXGlz7_"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "  \"models\": {\n",
        "    \"lstm\": {\n",
        "      \"embed_dim\": 32,\n",
        "      \"hidden_dim\": 96,\n",
        "      \"num_layers\": 1,\n",
        "      \"output_dim\": 1\n",
        "    },\n",
        "    \"s4\": {\n",
        "      \"embed_dim\": 24,\n",
        "      \"hidden_dim\": 128,\n",
        "      \"output_dim\": 1,\n",
        "      \"num_layers\": 2\n",
        "    },\n",
        "    \"transformer\": {\n",
        "      \"embed_dim\": 32,\n",
        "      \"num_heads\": 2,\n",
        "      \"hidden_dim\": 128,\n",
        "      \"output_dim\": 1,\n",
        "      \"num_layers\": 4\n",
        "    }\n",
        "  },\n",
        "  \"run_parameters\": {\n",
        "    \"batch_size\": 16,\n",
        "    \"n_epochs\": 50,\n",
        "    \"learning_rate\": 0.001\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paOmrocTl2Ju"
      },
      "source": [
        "Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dvePJXxl3Va"
      },
      "outputs": [],
      "source": [
        "# Setup basic configuration for logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout)  # Ensure logs are directed to stdout\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def setup_logger(name=__name__):\n",
        "    logger = logging.getLogger(name)\n",
        "    return logger\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22InhBYbl6Ye"
      },
      "source": [
        "Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DhoXleNl7Vh"
      },
      "outputs": [],
      "source": [
        "def replace_final_layer(model, config, model_name, device):\n",
        "    hidden_dim = config[\"models\"][model_name][\"hidden_dim\"]\n",
        "    embed_dim = config[\"models\"][model_name][\"embed_dim\"]\n",
        "    if model_name == \"lstm\":\n",
        "        output_dim = 1\n",
        "        model.Wy = torch.nn.Parameter(torch.empty(output_dim, hidden_dim).to(device))\n",
        "        model.by = torch.nn.Parameter(torch.zeros(output_dim, 1).to(device))\n",
        "        torch.nn.init.xavier_uniform_(model.Wy)\n",
        "    elif model_name == \"s4\":\n",
        "        output_dim = 1\n",
        "        model.decoder = torch.nn.Linear(hidden_dim, output_dim).to(device)\n",
        "    else:\n",
        "        output_dim = 1\n",
        "        model.fc_out = torch.nn.Linear(embed_dim, output_dim).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayIrsAyEl89H"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sey0XXal912"
      },
      "outputs": [],
      "source": [
        "possible_models = list(config[\"models\"].keys())\n",
        "run_types = [\n",
        "        \"task\",\n",
        "        \"lra_pretrain\",\n",
        "        \"wikitext_pretrain\",\n",
        "        \"task_finetune_lra_pretrain\",\n",
        "        \"task_finetune_wikitext_pretrain\",\n",
        "    ]\n",
        "\n",
        "\n",
        "@dataclass.dataclass\n",
        "class Args:\n",
        "    model: str\n",
        "    run_type: str\n",
        "    use_wandb: bool\n",
        "\n",
        "args = Args(model=\"lstm\", run_type=\"task\", use_wandb=False)\n",
        "\n",
        "model_config = config[\"models\"][args.model]\n",
        "run_parameters = config[\"run_parameters\"]\n",
        "\n",
        "# Setup logging\n",
        "logger = setup_logger(__name__)\n",
        "\n",
        "run_name = f\"run_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{args.model}_{args.run_type}\"\n",
        "\n",
        "batch_size, n_epochs, learning_rate = (\n",
        "    run_parameters[\"batch_size\"],\n",
        "    run_parameters[\"n_epochs\"],\n",
        "    run_parameters[\"learning_rate\"],\n",
        ")\n",
        "\n",
        "# Load tokenizer and data loaders\n",
        "if \"wikitext\" in args.run_type and \"finetune\" in args.run_type:\n",
        "    wikitext_dataset = WikiTextDataset(split=\"train\")\n",
        "    tokenizer = wikitext_dataset.tokenizer\n",
        "    imdb_dataset = IMDBDataset(tokenizer=tokenizer)\n",
        "    train_dataloader, test_dataloader = imdb_dataset.get_dataloaders(batch_size)\n",
        "elif \"wikitext\" in args.run_type:\n",
        "    wikitext_dataset = WikiTextDataset(split=\"train\")\n",
        "    tokenizer = wikitext_dataset.tokenizer\n",
        "    train_dataloader, test_dataloader = wikitext_dataset.get_dataloaders(batch_size)\n",
        "else:\n",
        "    imdb_dataset = IMDBDataset()\n",
        "    tokenizer = imdb_dataset.tokenizer\n",
        "    train_dataloader, test_dataloader = imdb_dataset.get_dataloaders(batch_size)\n",
        "\n",
        "# Initialize model, criterion, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "finetune = args.run_type in [\n",
        "    \"task\",\n",
        "    \"task_finetune_lra_pretrain\",\n",
        "    \"task_finetune_wikitext_pretrain\",\n",
        "]\n",
        "\n",
        "model_dic = {\n",
        "    \"lstm\": CustomLSTMModel,\n",
        "    \"s4\": S4Model,\n",
        "    \"transformer\": CustomTransformerModel,\n",
        "}\n",
        "\n",
        "if args.run_type == \"task\":\n",
        "    config[\"models\"][args.model][\"output_dim\"] = 1\n",
        "else:\n",
        "    config[\"models\"][args.model][\"output_dim\"] = vocab_size\n",
        "\n",
        "model = model_dic[args.model](\n",
        "    vocab_size=vocab_size, **config[\"models\"][args.model], finetune=finetune\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "pretrained = \"lra_pretrained\" if \"lra\" in args.run_type else \"wikitext_pretrained\"\n",
        "checkpoint_path = f\"{args.model}_{pretrained}.pth\"\n",
        "if finetune and args.run_type != \"task\":\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "    logger.info(f\"Checkpoint loaded from {checkpoint_path}\")\n",
        "    replace_final_layer(model, config, args.model, device)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss() if finetune else torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "logger.info(f\"parameter_cnt: {model.count_parameters}\")\n",
        "\n",
        "if args.use_wandb:\n",
        "    # Initialize WandB\n",
        "    wandb.login(key=\"5fda0926085bc8963be5e43c4e501d992e35abe8\")\n",
        "    wandb.init(project=\"model-comparison\", name=run_name)\n",
        "\n",
        "    # Log hyperparameters and model\n",
        "    wandb.config.update(\n",
        "        {\n",
        "            **{\n",
        "                \"run_name\": run_name,\n",
        "                \"model\": args.model,\n",
        "                \"parameter_cnt\": model.count_parameters,\n",
        "            },\n",
        "            **config[\"models\"][args.model],\n",
        "            **config[\"run_parameters\"],\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Training loop\n",
        "logger.info(\"Starting training...\")\n",
        "\n",
        "if finetune:\n",
        "    train, evaluate = \n",
        "else:\n",
        "    from train_evaluate.pretrain_train_evaluate import train, evaluate\n",
        "\n",
        "best_loss = float(\"inf\")\n",
        "epochs_since_improvement = 0\n",
        "\n",
        "# add seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(\n",
        "        model, train_dataloader, criterion, optimizer, device, epoch, logger\n",
        "    )\n",
        "    test_evaluate_res = evaluate(model, test_dataloader, criterion, device, logger)\n",
        "\n",
        "    test_accuracy = None\n",
        "    if not finetune:\n",
        "        test_loss = test_evaluate_res\n",
        "    else:\n",
        "        test_loss, test_accuracy = test_evaluate_res\n",
        "    logger.info(\n",
        "        f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy}\"\n",
        "    )\n",
        "\n",
        "    if args.use_wandb:\n",
        "        # Log metrics to WandB\n",
        "        log_dict = {\"train_loss\": train_loss, \"test_loss\": test_loss}\n",
        "        if test_accuracy is not None:\n",
        "            log_dict[\"test_accuracy\"] = test_accuracy\n",
        "        wandb.log(log_dict)\n",
        "\n",
        "    # Check for improvement\n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        epochs_since_improvement = 0\n",
        "        pretrained = (\n",
        "            \"lra_pretrained\" if \"lra\" in args.run_type else \"wikitext_pretrained\"\n",
        "        )\n",
        "        checkpoint_path = f\"{args.model}_{pretrained}.pth\"\n",
        "        if not finetune:\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            logger.info(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "        logger.info(\n",
        "            f\"New best model found at epoch {epoch} with test loss {test_loss:.4f}\"\n",
        "        )\n",
        "    else:\n",
        "        epochs_since_improvement += 1\n",
        "        logger.info(f\"No improvement for {epochs_since_improvement} epochs\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if epochs_since_improvement >= 5:\n",
        "        logger.info(f\"No improvement for 5 consecutive epochs. Stopping training.\")\n",
        "        break\n",
        "\n",
        "logger.info(\"Training completed.\")\n",
        "\n",
        "if args.use_wandb:\n",
        "    wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
